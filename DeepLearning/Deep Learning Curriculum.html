<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Complete Deep Learning & Computer Vision Curriculum</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --bg: #0f1419;
            --surface: #1a1f2e;
            --text: #e4e6eb;
            --text-dim: #b0b7c3;
            --cyan: #00d4ff;
            --orange: #ff6b35;
            --green: #00ff88;
            --yellow: #ffa500;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            overflow-x: hidden;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            text-align: center;
            margin-bottom: 40px;
            padding: 30px 0;
            border-bottom: 2px solid var(--cyan);
        }

        h1 {
            font-size: 2.5em;
            background: linear-gradient(135deg, var(--cyan), var(--orange));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: 10px;
        }

        .subtitle {
            color: var(--text-dim);
            font-size: 1.1em;
        }

        .dashboard {
            display: none;
        }

        .dashboard.active {
            display: block;
        }

        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 25px;
            margin: 40px 0;
        }

        .card {
            background: linear-gradient(135deg, rgba(0, 212, 255, 0.1), rgba(255, 107, 53, 0.1));
            border: 2px solid var(--cyan);
            border-radius: 12px;
            padding: 30px;
            cursor: pointer;
            transition: all 0.3s ease;
            text-align: center;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 212, 255, 0.2);
            border-color: var(--orange);
        }

        .card-icon {
            font-size: 3em;
            margin-bottom: 15px;
        }

        .card h3 {
            color: var(--cyan);
            font-size: 1.5em;
            margin-bottom: 10px;
        }

        .card p {
            color: var(--text-dim);
            font-size: 0.95em;
        }

        .category-label {
            display: inline-block;
            margin-top: 10px;
            padding: 5px 12px;
            background: rgba(0, 212, 255, 0.2);
            border-radius: 20px;
            font-size: 0.85em;
            color: var(--green);
        }

        .module {
            display: none;
        }

        .module.active {
            display: block;
            animation: fadeIn 0.3s ease;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
            }

            to {
                opacity: 1;
            }
        }

        .btn-back {
            padding: 10px 20px;
            background: var(--orange);
            color: var(--bg);
            border: none;
            border-radius: 6px;
            cursor: pointer;
            font-weight: 600;
            margin-bottom: 25px;
            transition: all 0.3s ease;
        }

        .btn-back:hover {
            background: var(--cyan);
        }

        .tabs {
            display: flex;
            gap: 10px;
            margin-bottom: 30px;
            flex-wrap: wrap;
            justify-content: center;
            border-bottom: 1px solid rgba(0, 212, 255, 0.2);
            padding-bottom: 15px;
            overflow-x: auto;
        }

        .tab-btn {
            padding: 10px 20px;
            background: var(--surface);
            color: var(--text);
            border: 2px solid transparent;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.95em;
            transition: all 0.3s ease;
            font-weight: 500;
            white-space: nowrap;
        }

        .tab-btn:hover {
            background: rgba(0, 212, 255, 0.1);
            border-color: var(--cyan);
        }

        .tab-btn.active {
            background: var(--cyan);
            color: var(--bg);
            border-color: var(--cyan);
        }

        .tab {
            display: none;
        }

        .tab.active {
            display: block;
            animation: fadeIn 0.3s ease;
        }

        .section {
            background: var(--surface);
            border: 1px solid rgba(0, 212, 255, 0.2);
            border-radius: 10px;
            padding: 30px;
            margin-bottom: 25px;
            transition: all 0.3s ease;
        }

        .section:hover {
            border-color: var(--cyan);
            box-shadow: 0 0 20px rgba(0, 212, 255, 0.1);
        }

        h2 {
            color: var(--cyan);
            font-size: 1.8em;
            margin-bottom: 15px;
        }

        h3 {
            color: var(--orange);
            font-size: 1.3em;
            margin-top: 20px;
            margin-bottom: 12px;
        }

        h4 {
            color: var(--green);
            font-size: 1.1em;
            margin-top: 15px;
            margin-bottom: 10px;
        }

        p {
            margin-bottom: 15px;
            line-height: 1.8;
        }

        ul {
            margin-left: 20px;
            margin-bottom: 15px;
        }

        ul li {
            margin-bottom: 8px;
        }

        .info-box {
            background: linear-gradient(135deg, rgba(0, 212, 255, 0.1), rgba(255, 107, 53, 0.1));
            border: 1px solid var(--cyan);
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
        }

        .box-title {
            color: var(--orange);
            font-weight: 700;
            margin-bottom: 10px;
            font-size: 1.1em;
        }

        .box-content {
            color: var(--text-dim);
            line-height: 1.7;
        }

        .formula {
            background: rgba(0, 212, 255, 0.1);
            border: 1px solid var(--cyan);
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            line-height: 1.8;
            color: var(--cyan);
        }

        .callout {
            border-left: 4px solid;
            padding: 15px;
            margin: 20px 0;
            border-radius: 6px;
        }

        .callout.tip {
            border-left-color: var(--green);
            background: rgba(0, 255, 136, 0.05);
        }

        .callout.warning {
            border-left-color: var(--yellow);
            background: rgba(255, 165, 0, 0.05);
        }

        .callout.insight {
            border-left-color: var(--cyan);
            background: rgba(0, 212, 255, 0.05);
        }

        .callout-title {
            font-weight: 700;
            margin-bottom: 8px;
        }

        .list-item {
            display: flex;
            gap: 12px;
            margin: 12px 0;
            padding: 12px;
            background: rgba(0, 212, 255, 0.05);
            border-left: 3px solid var(--cyan);
            border-radius: 4px;
        }

        .list-num {
            color: var(--orange);
            font-weight: 700;
            min-width: 30px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th,
        td {
            padding: 12px;
            text-align: left;
            border: 1px solid rgba(0, 212, 255, 0.2);
        }

        th {
            background: rgba(0, 212, 255, 0.1);
            color: var(--cyan);
            font-weight: 700;
        }

        .viz-container {
            background: rgba(0, 212, 255, 0.02);
            border: 1px solid rgba(0, 212, 255, 0.2);
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
            display: flex;
            justify-content: center;
            overflow-x: auto;
        }

        .viz-controls {
            display: flex;
            gap: 10px;
            margin-top: 20px;
            justify-content: center;
            flex-wrap: wrap;
        }

        .btn-viz {
            padding: 10px 20px;
            background: var(--cyan);
            color: var(--bg);
            border: none;
            border-radius: 6px;
            font-weight: 600;
            cursor: pointer;
            font-size: 0.95em;
            transition: all 0.3s ease;
        }

        .btn-viz:hover {
            background: var(--orange);
            transform: scale(1.05);
        }

        canvas {
            max-width: 100%;
            height: auto;
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.8em;
            }

            .tabs {
                flex-direction: column;
            }

            .tab-btn {
                width: 100%;
            }

            .grid {
                grid-template-columns: 1fr;
            }

            canvas {
                width: 100% !important;
                height: auto !important;
            }
        }
    </style>
</head>

<body>
    <div class="container">
        <!-- MAIN DASHBOARD -->
        <div id="dashboard" class="dashboard active">
            <header>
                <h1>üß† Complete Deep Learning & Computer Vision</h1>
                <p class="subtitle">Comprehensive Curriculum | Foundations to Advanced Applications</p>
            </header>

            <div style="text-align: center; margin-bottom: 40px;">
                <p style="color: var(--text-dim); font-size: 1.1em;">
                    Master all aspects of deep learning and computer vision. 25+ modules covering neural networks, CNNs,
                    object detection, GANs, and more.
                </p>
            </div>

            <div class="grid" id="modulesGrid"></div>
        </div>

        <!-- MODULES CONTAINER -->
        <div id="modulesContainer"></div>
    </div>

    <script>
        const modules = [
            // Module 1: Deep Learning Foundations
            {
                id: "nn-basics",
                title: "Introduction to Neural Networks",
                icon: "üß¨",
                category: "Foundations",
                color: "#0088ff",
                description: "Biological vs. Artificial neurons and network architecture"
            },
            {
                id: "perceptron",
                title: "The Perceptron",
                icon: "‚öôÔ∏è",
                category: "Foundations",
                color: "#0088ff",
                description: "Single layer networks and their limitations"
            },
            {
                id: "mlp",
                title: "Multi-Layer Perceptron (MLP)",
                icon: "üèóÔ∏è",
                category: "Foundations",
                color: "#0088ff",
                description: "Hidden layers and deep architectures"
            },
            {
                id: "activation",
                title: "Activation Functions",
                icon: "‚ö°",
                category: "Foundations",
                color: "#0088ff",
                description: "Sigmoid, ReLU, Tanh, Leaky ReLU, ELU, Softmax"
            },
            {
                id: "weight-init",
                title: "Weight Initialization",
                icon: "üéØ",
                category: "Foundations",
                color: "#0088ff",
                description: "Xavier, He, Random initialization strategies"
            },
            {
                id: "loss",
                title: "Loss Functions",
                icon: "üìâ",
                category: "Foundations",
                color: "#0088ff",
                description: "MSE, Binary Cross-Entropy, Categorical Cross-Entropy"
            },
            {
                id: "optimizers",
                title: "Optimizers",
                icon: "üéØ",
                category: "Training",
                color: "#00ff00",
                description: "SGD, Momentum, Adam, Adagrad, RMSprop"
            },
            {
                id: "backprop",
                title: "Forward & Backpropagation",
                icon: "‚¨ÖÔ∏è",
                category: "Training",
                color: "#00ff00",
                description: "Chain rule and gradient computation"
            },
            {
                id: "regularization",
                title: "Regularization",
                icon: "üõ°Ô∏è",
                category: "Training",
                color: "#00ff00",
                description: "L1/L2, Dropout, Early Stopping, Batch Norm"
            },
            {
                id: "batch-norm",
                title: "Batch Normalization",
                icon: "‚öôÔ∏è",
                category: "Training",
                color: "#00ff00",
                description: "Stabilizing and speeding up training"
            },
            // Module 2: Computer Vision Fundamentals
            {
                id: "cv-intro",
                title: "CV Fundamentals",
                icon: "üëÅÔ∏è",
                category: "Computer Vision",
                color: "#ff6b35",
                description: "Why ANNs fail with images, parameter explosion"
            },
            {
                id: "conv-layer",
                title: "Convolutional Layers",
                icon: "üñºÔ∏è",
                category: "Computer Vision",
                color: "#ff6b35",
                description: "Kernels, filters, feature maps, stride, padding"
            },
            {
                id: "pooling",
                title: "Pooling Layers",
                icon: "üì¶",
                category: "Computer Vision",
                color: "#ff6b35",
                description: "Max pooling, average pooling, spatial reduction"
            },
            {
                id: "cnn-basics",
                title: "CNN Architecture",
                icon: "üèóÔ∏è",
                category: "Computer Vision",
                color: "#ff6b35",
                description: "Combining conv, pooling, and fully connected layers"
            },
            {
                id: "viz-filters",
                title: "Visualizing CNNs",
                icon: "üîç",
                category: "Computer Vision",
                color: "#ff6b35",
                description: "What filters learn: edges ‚Üí shapes ‚Üí objects"
            },
            // Module 3: Advanced CNN Architectures
            {
                id: "lenet",
                title: "LeNet-5",
                icon: "üî¢",
                category: "CNN Architectures",
                color: "#ff00ff",
                description: "Classic digit recognizer (MNIST)"
            },
            {
                id: "alexnet",
                title: "AlexNet",
                icon: "üåü",
                category: "CNN Architectures",
                color: "#ff00ff",
                description: "The breakthrough in deep computer vision (2012)"
            },
            {
                id: "vgg",
                title: "VGGNet",
                icon: "üìä",
                category: "CNN Architectures",
                color: "#ff00ff",
                description: "VGG-16/19: Deep networks with small filters"
            },
            {
                id: "resnet",
                title: "ResNet",
                icon: "üåâ",
                category: "CNN Architectures",
                color: "#ff00ff",
                description: "Skip connections, solving vanishing gradients"
            },
            {
                id: "inception",
                title: "InceptionNet (GoogLeNet)",
                icon: "üéØ",
                category: "CNN Architectures",
                color: "#ff00ff",
                description: "1x1 convolutions, multi-scale feature extraction"
            },
            {
                id: "mobilenet",
                title: "MobileNet",
                icon: "üì±",
                category: "CNN Architectures",
                color: "#ff00ff",
                description: "Depth-wise separable convolutions for efficiency"
            },
            {
                id: "transfer-learning",
                title: "Transfer Learning",
                icon: "üîÑ",
                category: "CNN Architectures",
                color: "#ff00ff",
                description: "Fine-tuning and leveraging pre-trained models"
            },
            // Module 4: Object Detection & Segmentation
            {
                id: "localization",
                title: "Object Localization",
                icon: "üìç",
                category: "Detection",
                color: "#00ff00",
                description: "Bounding boxes and classification together"
            },
            {
                id: "rcnn",
                title: "R-CNN Family",
                icon: "üéØ",
                category: "Detection",
                color: "#00ff00",
                description: "R-CNN, Fast R-CNN, Faster R-CNN"
            },
            {
                id: "yolo",
                title: "YOLO",
                icon: "‚ö°",
                category: "Detection",
                color: "#00ff00",
                description: "Real-time object detection (v3, v5, v8)"
            },
            {
                id: "ssd",
                title: "SSD",
                icon: "üöÄ",
                category: "Detection",
                color: "#00ff00",
                description: "Single Shot MultiBox Detector"
            },
            {
                id: "semantic-seg",
                title: "Semantic Segmentation",
                icon: "üñåÔ∏è",
                category: "Segmentation",
                color: "#00ff00",
                description: "Pixel-level classification (U-Net)"
            },
            {
                id: "instance-seg",
                title: "Instance Segmentation",
                icon: "üë•",
                category: "Segmentation",
                color: "#00ff00",
                description: "Mask R-CNN and separate object instances"
            },
            {
                id: "face-recog",
                title: "Face Recognition",
                icon: "üë§",
                category: "Segmentation",
                color: "#00ff00",
                description: "Siamese networks and triplet loss"
            },
            // Module 5: Generative Models
            {
                id: "autoencoders",
                title: "Autoencoders",
                icon: "üîÄ",
                category: "Generative",
                color: "#ffaa00",
                description: "Encoder-decoder, latent space, denoising"
            },
            {
                id: "gans",
                title: "GANs (Generative Adversarial Networks)",
                icon: "üéÆ",
                category: "Generative",
                color: "#ffaa00",
                description: "Generator vs. Discriminator, DCGAN"
            },
            {
                id: "diffusion",
                title: "Diffusion Models",
                icon: "üåä",
                category: "Generative",
                color: "#ffaa00",
                description: "Foundation of Stable Diffusion and DALL-E"
            },
            // Additional Advanced Topics
            {
                id: "rnn",
                title: "RNNs & LSTMs",
                icon: "üîÑ",
                category: "Sequence",
                color: "#ff6b35",
                description: "Recurrent networks for sequential data"
            },
            {
                id: "transformers",
                title: "Transformers",
                icon: "üîó",
                category: "Sequence",
                color: "#ff6b35",
                description: "Attention mechanisms and modern architectures"
            },
            {
                id: "bert",
                title: "BERT & NLP Transformers",
                icon: "üìö",
                category: "NLP",
                color: "#ff6b35",
                description: "Bidirectional transformers for language"
            },
            {
                id: "gpt",
                title: "GPT & Language Models",
                icon: "üí¨",
                category: "NLP",
                color: "#ff6b35",
                description: "Autoregressive models and text generation"
            },
            {
                id: "vit",
                title: "Vision Transformers (ViT)",
                icon: "üé®",
                category: "Vision",
                color: "#ff6b35",
                description: "Transformers applied to image data"
            }
        ];

        // Comprehensive content for all modules
        const MODULE_CONTENT = {
            "nn-basics": {
                overview: `
                    <h3>What are Neural Networks?</h3>
                    <p>Neural Networks are computational models inspired by the human brain's structure. They consist of interconnected nodes (neurons) organized in layers that process information through weighted connections.</p>
                    
                    <h3>Why Use Neural Networks?</h3>
                    <ul>
                        <li><strong>Universal Approximation:</strong> Can theoretically approximate any continuous function</li>
                        <li><strong>Feature Learning:</strong> Automatically discover representations from raw data</li>
                        <li><strong>Adaptability:</strong> Learn from examples without explicit programming</li>
                        <li><strong>Parallel Processing:</strong> Highly parallelizable for modern hardware</li>
                    </ul>
                    
                    <div class="callout tip">
                        <div class="callout-title">‚úÖ Advantages</div>
                        ‚Ä¢ Non-linear problem solving<br>
                        ‚Ä¢ Robust to noisy data<br>
                        ‚Ä¢ Works with incomplete information<br>
                        ‚Ä¢ Continuous learning capability
                    </div>
                    
                    <div class="callout warning">
                        <div class="callout-title">‚ö†Ô∏è Disadvantages</div>
                        ‚Ä¢ Requires large amounts of training data<br>
                        ‚Ä¢ Computationally expensive<br>
                        ‚Ä¢ "Black box" - difficult to interpret<br>
                        ‚Ä¢ Prone to overfitting without regularization
                    </div>
                `,
                concepts: `
                    <h3>Core Components</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Neurons (Nodes):</strong> Basic computational units that receive inputs, apply weights, add bias, and apply activation function</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Layers:</strong> Input layer (receives data), Hidden layers (feature extraction), Output layer (predictions)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div><strong>Weights:</strong> Parameters learned during training that determine connection strength</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">04</div>
                        <div><strong>Bias:</strong> Allows shifting the activation function for better fitting</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">05</div>
                        <div><strong>Activation Function:</strong> Introduces non-linearity (ReLU, Sigmoid, Tanh)</div>
                    </div>
                `,
                applications: `
                    <h3>Real-World Applications</h3>
                    <div class="info-box">
                        <div class="box-title">üè• Healthcare</div>
                        <div class="box-content">Disease diagnosis, medical image analysis, drug discovery, patient risk prediction</div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üí∞ Finance</div>
                        <div class="box-content">Fraud detection, algorithmic trading, credit scoring, portfolio optimization</div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üõí E-commerce</div>
                        <div class="box-content">Recommendation systems, demand forecasting, customer segmentation, price optimization</div>
                    </div>
                `
            },
            "activation": {
                overview: `
                    <h3>What are Activation Functions?</h3>
                    <p>Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns. Without activation functions, a neural network would be just a linear regression model regardless of depth.</p>
                    
                    <h3>Why Do We Need Them?</h3>
                    <ul>
                        <li><strong>Non-linearity:</strong> Real-world problems are rarely linear</li>
                        <li><strong>Complex Pattern Learning:</strong> Enable learning of intricate decision boundaries</li>
                        <li><strong>Gradient Flow:</strong> Control how gradients propagate during backpropagation</li>
                        <li><strong>Range Normalization:</strong> Keep activations in manageable ranges</li>
                    </ul>
                    
                    <h3>Common Activation Functions Comparison</h3>
                    <table>
                        <tr>
                            <th>Function</th>
                            <th>Range</th>
                            <th>Best Use</th>
                            <th>Issue</th>
                        </tr>
                        <tr>
                            <td>ReLU</td>
                            <td>[0, ‚àû)</td>
                            <td>Hidden layers (default)</td>
                            <td>Dying ReLU problem</td>
                        </tr>
                        <tr>
                            <td>Sigmoid</td>
                            <td>(0, 1)</td>
                            <td>Binary classification output</td>
                            <td>Vanishing gradients</td>
                        </tr>
                        <tr>
                            <td>Tanh</td>
                            <td>(-1, 1)</td>
                            <td>RNNs, zero-centered</td>
                            <td>Vanishing gradients</td>
                        </tr>
                        <tr>
                            <td>Leaky ReLU</td>
                            <td>(-‚àû, ‚àû)</td>
                            <td>Fixes dying ReLU</td>
                            <td>Extra hyperparameter</td>
                        </tr>
                        <tr>
                            <td>Softmax</td>
                            <td>(0, 1) sum=1</td>
                            <td>Multi-class output</td>
                            <td>Computationally expensive</td>
                        </tr>
                    </table>
                `,
                concepts: `
                    <h3>Key Properties</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Differentiability:</strong> Must have derivatives for backpropagation to work</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Monotonicity:</strong> Preferably monotonic for easier optimization</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div><strong>Zero-Centered:</strong> Helps with faster convergence (Tanh)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">04</div>
                        <div><strong>Computational Efficiency:</strong> Should be fast to compute (ReLU wins)</div>
                    </div>
                    
                    <div class="callout tip">
                        <div class="callout-title">üí° Best Practices</div>
                        ‚Ä¢ Use <strong>ReLU</strong> for hidden layers by default<br>
                        ‚Ä¢ Use <strong>Sigmoid</strong> for binary classification output<br>
                        ‚Ä¢ Use <strong>Softmax</strong> for multi-class classification<br>
                        ‚Ä¢ Try <strong>Leaky ReLU</strong> or <strong>ELU</strong> if ReLU neurons are dying<br>
                        ‚Ä¢ Avoid Sigmoid/Tanh in deep networks (gradient vanishing)
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üß† Neural Network Design</div>
                        <div class="box-content">
                            Critical choice for every neural network - affects training speed, convergence, and final accuracy
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üéØ Task-Specific Selection</div>
                        <div class="box-content">
                            Different tasks need different outputs: Sigmoid for binary, Softmax for multi-class, Linear for regression
                        </div>
                    </div>
                `,
                math: `
                    <h3>Derivatives: The Backprop Fuel</h3>
                    <p>Activation functions must be differentiable for backpropagation to work. Let's look at the derivatives on paper:</p>
                    
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Sigmoid:</strong> œÉ(z) = 1 / (1 + e‚Åª·∂ª)<br>
                        <strong>Derivative:</strong> œÉ'(z) = œÉ(z)(1 - œÉ(z))<br>
                        <span class="formula-caption">Max gradient is 0.25 (at z=0). This is why deep networks vanish!</span></div>
                    </div>

                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Tanh:</strong> tanh(z) = (e·∂ª - e‚Åª·∂ª) / (e·∂ª + e‚Åª·∂ª)<br>
                        <strong>Derivative:</strong> tanh'(z) = 1 - tanh¬≤(z)<br>
                        <span class="formula-caption">Max gradient is 1.0 (at z=0). Better than Sigmoid, but still vanishes.</span></div>
                    </div>

                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div><strong>ReLU:</strong> max(0, z)<br>
                        <strong>Derivative:</strong> 1 if z > 0, else 0<br>
                        <span class="formula-caption">Gradient is 1.0 for all positive z. No vanishing! But 0 for negative (Dying ReLU).</span></div>
                    </div>

                    <div class="callout insight">
                        <div class="callout-title">üìù Paper & Pain: The Chain Effect</div>
                        Each layer multiplies the gradient by œÉ'(z). <br>
                        For 10 Sigmoid layers: Total gradient ‚âà (0.25)¬π‚Å∞ ‚âà <strong>0.00000095</strong><br>
                        This is the mathematical proof of the Vanishing Gradient Problem!
                    </div>
                `
            },
            "conv-layer": {
                overview: `
                    <h3>What are Convolutional Layers?</h3>
                    <p>Convolutional layers are the fundamental building blocks of CNNs. They apply learnable filters (kernels) across input data to detect local patterns like edges, textures, and shapes.</p>
                    
                    <h3>Why Use Convolutions Instead of Fully Connected Layers?</h3>
                    <ul>
                        <li><strong>Parameter Efficiency:</strong> Share weights across spatial locations (fewer parameters)</li>
                        <li><strong>Translation Invariance:</strong> Detect features regardless of position</li>
                        <li><strong>Local Connectivity:</strong> Each neuron sees

 only a small region (receptive field)</li>
                        <li><strong>Hierarchical Learning:</strong> Build complex features from simple ones</li>
                    </ul>
                    
                    <div class="callout insight">
                        <div class="callout-title">üîç Example: Parameter Comparison</div>
                        For a 224√ó224 RGB image:<br>
                        ‚Ä¢ <strong>Fully Connected:</strong> 224 √ó 224 √ó 3 √ó 1000 = 150M parameters (for 1000 neurons)<br>
                        ‚Ä¢ <strong>Convolutional (3√ó3):</strong> 3 √ó 3 √ó 3 √ó 64 = 1,728 parameters (for 64 filters)<br>
                        <strong>Result:</strong> 87,000x fewer parameters! üöÄ
                    </div>
                    
                    <div class="callout tip">
                        <div class="callout-title">‚úÖ Advantages</div>
                        ‚Ä¢ Drastically reduced parameters<br>
                        ‚Ä¢ Spatial hierarchy (edges ‚Üí textures ‚Üí parts ‚Üí objects)<br>
                        ‚Ä¢ GPU-friendly (highly parallelizable)<br>
                        ‚Ä¢ Built-in translation equivariance
                    </div>
                    
                    <div class="callout warning">
                        <div class="callout-title">‚ö†Ô∏è Disadvantages</div>
                        ‚Ä¢ Not rotation invariant (require data augmentation)<br>
                        ‚Ä¢ Fixed receptive field size<br>
                        ‚Ä¢ Memory intensive during training<br>
                        ‚Ä¢ Require careful hyperparameter tuning (kernel size, stride, padding)
                    </div>
                `,
                concepts: `
                    <h3>Key Hyperparameters</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Kernel/Filter Size:</strong> Typically 3√ó3 or 5√ó5. Smaller = more layers needed, larger = more parameters</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Stride:</strong> Step size when sliding filter. Stride=1 (preserves size), Stride=2 (downsamples by 2√ó)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div><strong>Padding:</strong> Add zeros around borders. 'SAME' keeps size, 'VALID' shrinks output</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">04</div>
                        <div><strong>Number of Filters:</strong> Each filter learns different features. More filters = more capacity but slower</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">05</div>
                        <div><strong>Dilation:</strong> Spacing between kernel elements. Increases receptive field without adding parameters</div>
                    </div>

                    <div class="formula">
                        Output Size Formula:<br>
                        W_out = floor((W_in + 2√ópadding - kernel_size) / stride) + 1<br>
                        H_out = floor((H_in + 2√ópadding - kernel_size) / stride) + 1
                    </div>
                `
            },
            "yolo": {
                overview: `
                    <h3>What is YOLO?</h3>
                    <p>YOLO (You Only Look Once) treats object detection as a single regression problem, going directly from image pixels to bounding box coordinates and class probabilities in one forward pass.</p>
                    
                    <h3>Why YOLO Over R-CNN?</h3>
                    <ul>
                        <li><strong>Speed:</strong> 45+ FPS (real-time) vs R-CNN's ~0.05 FPS</li>
                        <li><strong>Global Context:</strong> Sees entire image during training (fewer background errors)</li>
                        <li><strong>One Network:</strong> Unlike R-CNN's multi-stage pipeline</li>
                        <li><strong>End-to-End Training:</strong> Optimize detection directly</li>
                    </ul>
                    
                    <div class="callout tip">
                        <div class="callout-title">‚úÖ Advantages</div>
                        ‚Ä¢ <strong>Lightning Fast:</strong> Real-time inference (YOLOv8 at 100+ FPS)<br>
                        ‚Ä¢ <strong>Simple Architecture:</strong> Single network, easy to train<br>
                        ‚Ä¢ <strong>Generalizes Well:</strong> Works on natural images and artwork<br>
                        ‚Ä¢ <strong>Small Model Size:</strong> Can run on edge devices (mobile, IoT)
                    </div>
                    
                    <div class="callout warning">
                        <div class="callout-title">‚ö†Ô∏è Disadvantages</div>
                        ‚Ä¢ <strong>Struggles with Small Objects:</strong> Grid limitation affects tiny items<br>
                        ‚Ä¢ <strong>Localization Errors:</strong> Less precise than two-stage detectors<br>
                        ‚Ä¢ <strong>Limited Objects per Cell:</strong> Can't detect many close objects<br>
                        ‚Ä¢ <strong>Aspect Ratio Issues:</strong> Struggles with unusual object shapes
                    </div>
                    
                    <h3>YOLO Evolution</h3>
                    <table>
                        <tr>
                            <th>Version</th>
                            <th>Year</th>
                            <th>Key Innovation</th>
                            <th>mAP</th>
                        </tr>
                        <tr>
                            <td>YOLOv1</td>
                            <td>2015</td>
                            <td>Original single-shot detector</td>
                            <td>63.4%</td>
                        </tr>
                        <tr>
                            <td>YOLOv3</td>
                            <td>2018</td>
                            <td>Multi-scale predictions</td>
                            <td>57.9% (faster)</td>
                        </tr>
                        <tr>
                            <td>YOLOv5</td>
                            <td>2020</td>
                            <td>PyTorch, Auto-augment</td>
                            <td>~50% (optimized)</td>
                        </tr>
                        <tr>
                            <td>YOLOv8</td>
                            <td>2023</td>
                            <td>Anchor-free, SOTA speed</td>
                            <td>53.9% (real-time)</td>
                        </tr>
                    </table>
                `,
                concepts: `
                    <h3>How YOLO Works (3 Steps)</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Grid Division:</strong> Divide image into S√óS grid (e.g., 7√ó7). Each cell predicts B bounding boxes</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Predictions Per Cell:</strong> Each box predicts (x, y, w, h, confidence) + class probabilities</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div><strong>Non-Max Suppression:</strong> Remove duplicate detections, keep highest confidence boxes</div>
                    </div>
                    
                    <div class="formula">
                        Output Tensor Shape (YOLOv1):<br>
                        S √ó S √ó (B √ó 5 + C)<br>
                        Example: 7 √ó 7 √ó (2 √ó 5 + 20) = 7 √ó 7 √ó 30<br>
                        <br>
                        Where:<br>
                        ‚Ä¢ S = grid size (7)<br>
                        ‚Ä¢ B = boxes per cell (2)<br>
                        ‚Ä¢ 5 = (x, y, w, h, confidence)<br>
                        ‚Ä¢ C = number of classes (20 for PASCAL VOC)
                    </div>
                `,
                applications: `
                    <h3>Industry Applications</h3>
                    <div class="info-box">
                        <div class="box-title">üöó Autonomous Vehicles</div>
                        <div class="box-content">
                            Real-time detection of pedestrians, vehicles, traffic signs, and lane markings for self-driving cars
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üè≠ Manufacturing</div>
                        <div class="box-content">
                            Quality control, defect detection on assembly lines, robot guidance, inventory management
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üõ°Ô∏è Security & Surveillance</div>
                        <div class="box-content">
                            Intrusion detection, crowd monitoring, suspicious behavior analysis, license plate recognition
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üè• Medical Imaging</div>
                        <div class="box-content">
                            Tumor localization, cell counting, anatomical structure detection in X-rays/CT scans
                        </div>
                    </div>
                `
            },
            "transformers": {
                overview: `
                    <h3>What are Transformers?</h3>
                    <p>Transformers are neural architectures based entirely on attention mechanisms, eliminating recurrence and convolutions. Introduced in "Attention is All You Need" (2017), they revolutionized NLP and are now conquering computer vision.</p>
                    
                    <h3>Why Transformers Over RNNs/LSTMs?</h3>
                    <ul>
                        <li><strong>Parallelization:</strong> Process entire sequence at once (vs sequential RNNs)</li>
                        <li><strong>Long-Range Dependencies:</strong> Direct connections between any two positions</li>
                        <li><strong>No Gradient Vanishing:</strong> Skip connections and attention bypass depth issues</li>
                        <li><strong>Scalability:</strong> Performance improves with more data and compute</li>
                    </ul>
                    
                    <div class="callout tip">
                        <div class="callout-title">‚úÖ Advantages</div>
                        ‚Ä¢ <strong>Superior Performance:</strong> SOTA on nearly all NLP benchmarks<br>
                        ‚Ä¢ <strong>Highly Parallelizable:</strong> Train 100√ó faster than RNNs on TPUs/GPUs<br>
                        ‚Ä¢ <strong>Transfer Learning:</strong> Pre-train once, fine-tune for many tasks<br>
                        ‚Ä¢ <strong>Interpretability:</strong> Attention weights show what model focuses on<br>
                        ‚Ä¢ <strong>Multi-Modal:</strong> Works for text, images, audio, video
                    </div>
                    
                    <div class="callout warning">
                        <div class="callout-title">‚ö†Ô∏è Disadvantages</div>
                        ‚Ä¢ <strong>Quadratic Complexity:</strong> O(n¬≤) in sequence length (memory intensive)<br>
                        ‚Ä¢ <strong>Massive Data Requirements:</strong> Need millions of examples to train from scratch<br>
                        ‚Ä¢ <strong>Computational Cost:</strong> Training GPT-3 cost ~$4.6M<br>
                        ‚Ä¢ <strong>Position Encoding:</strong> Require explicit positional information<br>
                        ‚Ä¢ <strong>Limited Context:</strong> Most models cap at 512-4096 tokens
                    </div>
                    
                    <h3>Transformer Variants</h3>
                    <table>
                        <tr>
                            <th>Model</th>
                            <th>Type</th>
                            <th>Architecture</th>
                            <th>Best For</th>
                        </tr>
                        <tr>
                            <td>BERT</td>
                            <td>Encoder-only</td>
                            <td>Bidirectional</td>
                            <td>Understanding (classification, QA)</td>
                        </tr>
                        <tr>
                            <td>GPT</td>
                            <td>Decoder-only</td>
                            <td>Autoregressive</td>
                            <td>Generation (text, code)</td>
                        </tr>
                        <tr>
                            <td>T5</td>
                            <td>Encoder-Decoder</td>
                            <td>Full Transformer</td>
                            <td>Text-to-text tasks (translation)</td>
                        </tr>
                        <tr>
                            <td>ViT</td>
                            <td>Encoder-only</td>
                            <td>Patch embeddings</td>
                            <td>Image classification</td>
                        </tr>
                    </table>
                `,
                concepts: `
                    <h3>Core Components</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Self-Attention:</strong> Each token attends to all other tokens, learning contextual relationships</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Multi-Head Attention:</strong> Multiple attention mechanisms in parallel (8-16 heads), each learning different patterns</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div><strong>Positional Encoding:</strong> Add position information since attention is permutation-invariant</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">04</div>
                        <div><strong>Feed-Forward Networks:</strong> Two-layer MLPs applied to each position independently</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">05</div>
                        <div><strong>Layer Normalization:</strong> Stabilize training, applied before attention and FFN</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">06</div>
                        <div><strong>Residual Connections:</strong> Skip connections around each sub-layer for gradient flow</div>
                    </div>
                    
                    <div class="formula">
                        Self-Attention Formula:<br>
                        Attention(Q, K, V) = softmax(QK<sup>T</sup> / ‚àöd<sub>k</sub>) V<br>
                        <br>
                        Where:<br>
                        ‚Ä¢ Q = Queries (what we're looking for)<br>
                        ‚Ä¢ K = Keys (what each token represents)<br>
                        ‚Ä¢ V = Values (actual information to aggregate)<br>
                        ‚Ä¢ d<sub>k</sub> = dimension of keys (for scaling)<br>
                        <br>
                        Multi-Head Attention:<br>
                        MultiHead(Q,K,V) = Concat(head‚ÇÅ,...,head<sub>h</sub>)W<sup>O</sup><br>
                        where head<sub>i</sub> = Attention(QW<sub>i</sub><sup>Q</sup>, KW<sub>i</sub><sup>K</sup>, VW<sub>i</sub><sup>V</sup>)
                    </div>
                `,
                applications: `
                    <h3>Revolutionary Applications</h3>
                    <div class="info-box">
                        <div class="box-title">üí¨ Large Language Models</div>
                        <div class="box-content">
                            <strong>ChatGPT, GPT-4, Claude:</strong> Conversational AI, code generation, creative writing, reasoning<br>
                            <strong>BERT, RoBERTa:</strong> Search engines (Google), question answering, sentiment analysis
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üåê Machine Translation</div>
                        <div class="box-content">
                            <strong>Google Translate, DeepL:</strong> Transformers achieved human-level translation quality<br>
                            Supports 100+ languages, real-time translation
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üé® Multi-Modal AI</div>
                        <div class="box-content">
                            <strong>DALL-E, Midjourney:</strong> Text-to-image generation<br>
                            <strong>CLIP:</strong> Image-text understanding<br>
                            <strong>Whisper:</strong> Speech recognition
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üß¨ Scientific Discovery</div>
                        <div class="box-content">
                            <strong>AlphaFold:</strong> Protein structure prediction (Nobel Prize-worthy breakthrough)<br>
                            <strong>Drug Discovery:</strong> Molecule generation and property prediction
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üíª Code Intelligence</div>
                        <div class="box-content">
                            <strong>GitHub Copilot:</strong> AI pair programmer<br>
                            <strong>CodeGen, AlphaCode:</strong> Automated coding, bug detection
                        </div>
                    </div>
                `,
                math: `
                    <h3>Scaled Dot-Product Attention</h3>
                    <p>The "heart" of the Transformer. It computes how much "attention" to pay to different parts of the input sequence.</p>
                    
                    <div class="formula" style="font-size: 1.3rem; text-align: center; margin: 20px 0; background: rgba(0, 212, 255, 0.05); padding: 20px; border-radius: 8px;">
                        Attention(Q, K, V) = softmax( (QK·µÄ) / ‚àöd‚Çñ ) V
                    </div>

                    <h3>Step-by-Step Derivation</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Dot Product (QK·µÄ):</strong> Compute raw similarity scores between Queries (what we want) and Keys (what we have)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Scaling (1/‚àöd‚Çñ):</strong> Divide by square root of key dimension. <strong>Why?</strong> With high dimensions, dot products grow large, pushing softmax into regions with vanishing gradients. Scaling prevents this.</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div><strong>Softmax:</strong> Convert similarity scores into probabilities (attention weights) that sum to 1</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">04</div>
                        <div><strong>Weighted Sum (√óV):</strong> Use attention weights to pull information from Values.</div>
                    </div>

                    <div class="callout insight">
                        <div class="callout-title">üìù Paper & Pain: Multi-Head Attention</div>
                        Instead of one big attention, we split Q, K, V into <em>h</em> heads:<br>
                        1. Heads learn <strong>different aspects</strong> (e.g., syntax vs semantics)<br>
                        2. Concat all heads: MultiHead = Concat(head‚ÇÅ, ..., head‚Çï)W·¥º<br>
                        3. Complexity: <strong>O(n¬≤ ¬∑ d)</strong> - This is why long sequences are hard!
                    </div>

                    <div class="callout warning">
                        <div class="callout-title">üìê Sinusoidal Positional Encoding</div>
                        PE(pos, 2i) = sin(pos / 10000^{2i/d})<br>
                        PE(pos, 2i+1) = cos(pos / 10000^{2i/d})<br>
                        This allows the model to learn relative positions since PE(pos+k) is a linear function of PE(pos).
                    </div>
                `
            },
            "perceptron": {
                overview: `
                    <h3>What is a Perceptron?</h3>
                    <p>The perceptron is the simplest neural network, invented in 1958. It's a binary linear classifier that makes predictions based on weighted inputs.</p>
                    
                    <div class="callout tip">
                        <div class="callout-title">‚úÖ Advantages</div>
                        ‚Ä¢ Simple and fast<br>
                        ‚Ä¢ Guaranteed convergence for linearly separable data<br>
                        ‚Ä¢ Interpretable weights
                    </div>
                    
                    <div class="callout warning">
                        <div class="callout-title">‚ö†Ô∏è Key Limitation</div>
                        <strong>Cannot solve XOR:</strong> Limited to linear decision boundaries only
                    </div>
                `,
                concepts: `
                    <h3>How Perceptron Works</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Weighted Sum:</strong> z = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + b</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Step Function:</strong> Output = 1 if z ‚â• 0, else 0</div>
                    </div>
                    <div class="formula">
                        Learning Rule: w_new = w_old + Œ±(y_true - y_pred)x
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üìö Educational</div>
                        <div class="box-content">
                            Historical importance - first trainable neural model. Perfect for teaching ML fundamentals
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üî¨ Simple Classification</div>
                        <div class="box-content">
                            Linearly separable problems: basic pattern recognition, simple binary decisions
                        </div>
                    </div>
                `
            },
            "mlp": {
                overview: `
                    <h3>Multi-Layer Perceptron (MLP)</h3>
                    <p>MLP adds hidden layers between input and output, enabling non-linear decision boundaries and solving the XOR problem that single perceptrons cannot.</p>
                    
                    <h3>Why MLPs?</h3>
                    <ul>
                        <li><strong>Universal Approximation:</strong> Can approximate any continuous function</li>
                        <li><strong>Non-Linear Learning:</strong> Solves complex problems</li>
                        <li><strong>Feature Extraction:</strong> Hidden layers learn hierarchical features</li>
                    </ul>
                    
                    <div class="callout insight">
                        <div class="callout-title">üí° The XOR Breakthrough</div>
                        Single perceptron: Cannot solve XOR<br>
                        MLP with 1 hidden layer (2 neurons): Solves XOR!<br>
                        This proves the power of depth.
                    </div>
                `,
                concepts: `
                    <h3>Architecture Components</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Input Layer:</strong> Raw features (no computation)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Hidden Layers:</strong> Extract progressively abstract features</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div><strong>Output Layer:</strong> Final predictions</div>
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üìä Tabular Data</div>
                        <div class="box-content">Credit scoring, fraud detection, customer churn, sales forecasting</div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üè≠ Manufacturing</div>
                        <div class="box-content">Quality control, predictive maintenance, demand forecasting</div>
                    </div>
                `,
                math: `
                    <h3>Neural Network Forward Pass (Matrix Form)</h3>
                    <p>Vectorization is key to modern deep learning. We process entire layers as matrix multiplications.</p>
                    
                    <div class="formula">
                        Layer 1: z‚ÅΩ¬π‚Åæ = W‚ÅΩ¬π‚Åæx + b‚ÅΩ¬π‚Åæ | a‚ÅΩ¬π‚Åæ = œÉ(z‚ÅΩ¬π‚Åæ)<br>
                        Layer 2: z‚ÅΩ¬≤‚Åæ = W‚ÅΩ¬≤‚Åæa‚ÅΩ¬π‚Åæ + b‚ÅΩ¬≤‚Åæ | a‚ÅΩ¬≤‚Åæ = œÉ(z‚ÅΩ¬≤‚Åæ)<br>
                        ...<br>
                        Layer L: ≈∑ = Softmax(W‚ÅΩ·¥∏‚Åæa‚ÅΩ·¥∏‚Åª¬π‚Åæ + b‚ÅΩ·¥∏‚Åæ)
                    </div>

                    <h3>Paper & Pain: Dimensionality Audit</h3>
                    <p>Understanding tensor shapes is the #1 skill for debugging neural networks.</p>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Input x:</strong> [n_features, 1]</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Weights W‚ÅΩ¬π‚Åæ:</strong> [n_hidden, n_features]</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div><strong>Bias b‚ÅΩ¬π‚Åæ:</strong> [n_hidden, 1]</div>
                    </div>

                    <div class="callout insight">
                        <div class="callout-title">üìù Paper & Pain: Solving XOR</div>
                        Input: [0,1], Target: 1<br>
                        Layer 1 (2 neurons):<br>
                        z‚ÇÅ = 10x‚ÇÅ + 10x‚ÇÇ - 5 &nbsp; | &nbsp; a‚ÇÅ = œÉ(z‚ÇÅ)<br>
                        z‚ÇÇ = 10x‚ÇÅ + 10x‚ÇÇ - 15 | &nbsp; a‚ÇÇ = œÉ(z‚ÇÇ)<br>
                        Layer 2 (1 neuron):<br>
                        y = œÉ(20a‚ÇÅ - 20a‚ÇÇ - 10)<br>
                        <strong>Try it on paper!</strong> This specific configuration correctly outputs XOR values.
                    </div>
                `
            },
            "weight-init": {
                overview: `
                    <h3>Weight Initialization Strategies</h3>
                    <table>
                        <tr>
                            <th>Method</th>
                            <th>Best For</th>
                            <th>Formula</th>
                        </tr>
                        <tr>
                            <td>Xavier/Glorot</td>
                            <td>Sigmoid, Tanh</td>
                            <td>N(0, ‚àö(2/(n_in+n_out)))</td>
                        </tr>
                        <tr>
                            <td>He/Kaiming</td>
                            <td>ReLU</td>
                            <td>N(0, ‚àö(2/n_in))</td>
                        </tr>
                    </table>
                    
                    <div class="callout warning">
                        <div class="callout-title">‚ö†Ô∏è Never Initialize to Zero!</div>
                        All neurons learn identical features (symmetry problem)
                    </div>
                `,
                concepts: `
                    <h3>Key Principles</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Variance Preservation:</strong> Keep activation variance similar across layers</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Symmetry Breaking:</strong> Different weights force different features</div>
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üéØ Critical for Deep Networks</div>
                        <div class="box-content">
                            Proper initialization is essential for training networks >10 layers. Wrong init = training failure
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">‚ö° Faster Convergence</div>
                        <div class="box-content">
                            Good initialization reduces training time by 2-10√ó, especially with modern optimizers
                        </div>
                    </div>
                `,
                math: `
                    <h3>The Variance Preservation Principle</h3>
                    <p>To prevent gradients from vanishing or exploding, we want the variance of the activations to remain constant across layers.</p>
                    
                    <div class="formula">
                        For a linear layer: y = Œ£ w·µ¢x·µ¢<br>
                        Var(y) = Var(Œ£ w·µ¢x·µ¢) = Œ£ Var(w·µ¢x·µ¢)<br>
                        Assuming w and x are independent with mean 0:<br>
                        Var(w·µ¢x·µ¢) = E[w·µ¢¬≤]E[x·µ¢¬≤] - E[w·µ¢]¬≤E[x·µ¢]¬≤ = Var(w·µ¢)Var(x·µ¢)<br>
                        So, Var(y) = n_in √ó Var(w) √ó Var(x)
                    </div>

                    <h3>1. Xavier (Glorot) Initialization</h3>
                    <p>Goal: Var(y) = Var(x) and Var(grad_out) = Var(grad_in)</p>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Forward Pass:</strong> n_in √ó Var(w) = 1  ‚áí Var(w) = 1/n_in</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Backward Pass:</strong> n_out √ó Var(w) = 1 ‚áí Var(w) = 1/n_out</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div><strong>Compromise:</strong> Var(w) = 2 / (n_in + n_out)</div>
                    </div>

                    <h3>2. He (Kaiming) Initialization</h3>
                    <p>For ReLU activation, half the neurons are inactive (output 0), which halves the variance. We must compensate.</p>
                    <div class="formula">
                        Var(ReLU(y)) = 1/2 √ó Var(y)<br>
                        To keep Var(ReLU(y)) = Var(x):<br>
                        1/2 √ó n_in √ó Var(w) = 1<br>
                        <strong>Var(w) = 2 / n_in</strong>
                    </div>

                    <div class="callout insight">
                        <div class="callout-title">üìù Paper & Pain Calculation</div>
                        If n_in = 256 and you use ReLU:<br>
                        Weight Std Dev = ‚àö(2/256) = ‚àö(1/128) ‚âà <strong>0.088</strong><br>
                        Initializing with std=1.0 or std=0.01 would cause immediate failure in a deep net!
                    </div>
                `
            },
            "loss": {
                overview: `
                    <h3>Loss Functions Guide</h3>
                    <table>
                        <tr>
                            <th>Task</th>
                            <th>Loss Function</th>
                        </tr>
                        <tr>
                            <td>Binary Classification</td>
                            <td>Binary Cross-Entropy</td>
                        </tr>
                        <tr>
                            <td>Multi-class</td>
                            <td>Categorical Cross-Entropy</td>
                        </tr>
                        <tr>
                            <td>Regression</td>
                            <td>MSE or MAE</td>
                        </tr>
                    </table>
                `,
                concepts: `
                    <h3>Common Loss Functions</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>MSE:</strong> (1/n)Œ£(y - ≈∑)¬≤ - Penalizes large errors</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Cross-Entropy:</strong> -Œ£(y¬∑log(≈∑)) - For classification</div>
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üéØ Task-Dependent Selection</div>
                        <div class="box-content">
                            Every ML task needs appropriate loss: classification (cross-entropy), regression (MSE/MAE), ranking (triplet loss)
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üìä Custom Losses</div>
                        <div class="box-content">
                            Business-specific objectives: Focal Loss (imbalanced data), Dice Loss (segmentation), Contrastive Loss (similarity learning)
                        </div>
                    </div>
                `,
                math: `
                    <h3>Binary Cross-Entropy (BCE) Derivation</h3>
                    <p>Why do we use logs? BCE is derived from Maximum Likelihood Estimation (MLE) assuming a Bernoulli distribution.</p>
                    
                    <div class="formula">
                        L(≈∑, y) = -(y log(≈∑) + (1-y) log(1-≈∑))
                    </div>

                    <h3>Paper & Pain: Why not MSE for Classification?</h3>
                    <p>If we use MSE for sigmoid output, the gradient is:</p>
                    <div class="formula">
                        ‚àÇL/‚àÇw = (≈∑ - y) <strong>œÉ'(z)</strong> x
                    </div>
                    <div class="callout warning">
                        <div class="callout-title">‚ö†Ô∏è The Saturation Problem</div>
                        If the model is very wrong (e.g., target 1, output 0.001), œÉ'(z) is near 0. <br>
                        The gradient vanishes, and the model <strong>stops learning!</strong>.
                    </div>

                    <h3>The BCE Advantage</h3>
                    <p>When using BCE, the œÉ'(z) term cancels out! The gradient becomes:</p>
                    <div class="formula" style="font-size: 1.2rem; color: #00d4ff;">
                        ‚àÇL/‚àÇw = (≈∑ - y) x
                    </div>
                    <div class="list-item">
                        <div class="list-num">üí°</div>
                        <div>This is beautiful: the gradient depends <strong>only on the error</strong> (≈∑-y), not on how saturated the neuron is. This enables much faster training.</div>
                    </div>
                `
            },
            "optimizers": {
                overview: `
                    <h3>Optimizer Selection Guide</h3>
                    <table>
                        <tr>
                            <th>Optimizer</th>
                            <th>When to Use</th>
                        </tr>
                        <tr>
                            <td>Adam/AdamW</td>
                            <td><strong>Default choice</strong> - works 90% of time</td>
                        </tr>
                        <tr>
                            <td>SGD + Momentum</td>
                            <td>CNNs (better final accuracy with patience)</td>
                        </tr>
                        <tr>
                            <td>RMSprop</td>
                            <td>RNNs</td>
                        </tr>
                    </table>
                    
                    <div class="formula">
                        Adam: m_t = Œ≤‚ÇÅ¬∑m + (1-Œ≤‚ÇÅ)¬∑‚àáL<br>
                        v_t = Œ≤‚ÇÇ¬∑v + (1-Œ≤‚ÇÇ)¬∑(‚àáL)¬≤<br>
                        w = w - Œ±¬∑m_t/‚àö(v_t)
                    </div>
                `,
                concepts: `
                    <h3>Optimizer Evolution</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>SGD:</strong> Simple but requires careful learning rate tuning</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Adam:</strong> Adaptive rates + momentum = works out-of-box</div>
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üöÄ Training Acceleration</div>
                        <div class="box-content">
                            Modern optimizers (Adam) reduce training time by 5-10√ó compared to basic SGD
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üéØ Architecture-Specific</div>
                        <div class="box-content">
                            CNNs: SGD+Momentum | Transformers: AdamW | RNNs: RMSprop | Default: Adam
                        </div>
                    </div>
                `
            },
            "backprop": {
                overview: `
                    <h3>Backpropagation Algorithm</h3>
                    <p>Backprop efficiently computes gradients by applying the chain rule from output to input, enabling training of deep networks.</p>
                    
                    <h3>Why Backpropagation?</h3>
                    <ul>
                        <li><strong>Efficient:</strong> Computes all gradients in single backward pass</li>
                        <li><strong>Scalable:</strong> Works for networks of any depth</li>
                        <li><strong>Automatic:</strong> Modern frameworks do it automatically</li>
                    </ul>
                `,
                concepts: `
                    <div class="formula">
                        Chain Rule:<br>
                        ‚àÇL/‚àÇw = ‚àÇL/‚àÇy √ó ‚àÇy/‚àÇz √ó ‚àÇz/‚àÇw<br>
                        <br>
                        For layer l:<br>
                        Œ¥À° = (W^(l+1))^T Œ¥^(l+1) ‚äô œÉ'(z^l)<br>
                        ‚àÇL/‚àÇW^l = Œ¥^l (a^(l-1))^T
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üß† Universal Training Method</div>
                        <div class="box-content">
                            Every modern neural network uses backprop - from CNNs to Transformers to GANs
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üîß Automatic Differentiation</div>
                        <div class="box-content">
                            PyTorch, TensorFlow implement automatic backprop - you define forward pass, framework does backward
                        </div>
                    </div>
                `,
                math: `
                    <h3>The 4 Fundamental Equations of Backprop</h3>
                    <p>Backpropagation is essentially the chain rule applied iteratively. We define the error signal Œ¥ = ‚àÇL/‚àÇz.</p>
                    
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Error at Output Layer (L):</strong><br>
                        Œ¥·¥∏ = ‚àá‚ÇêL ‚äô œÉ'(z·¥∏)<br>
                        <span class="formula-caption">Example for MSE: (a·¥∏ - y) ‚äô œÉ'(z·¥∏)</span></div>
                    </div>

                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Error at Layer l (Backwards):</strong><br>
                        Œ¥À° = ((WÀ°‚Å∫¬π)·µÄ Œ¥À°‚Å∫¬π) ‚äô œÉ'(zÀ°)</div>
                    </div>

                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div><strong>Gradient w.r.t Bias:</strong><br>
                        ‚àÇL / ‚àÇbÀ° = Œ¥À°</div>
                    </div>

                    <div class="list-item">
                        <div class="list-num">04</div>
                        <div><strong>Gradient w.r.t Weights:</strong><br>
                        ‚àÇL / ‚àÇWÀ° = Œ¥À° (aÀ°‚Åª¬π)·µÄ</div>
                    </div>

                    <div class="callout insight">
                        <div class="callout-title">üìù Paper & Pain Walkthrough</div>
                        Suppose single neuron: z = wx + b, Loss L = (œÉ(z) - y)¬≤/2<br>
                        1. <strong>Forward:</strong> z=2, a=œÉ(2)‚âà0.88, y=1, L=0.007<br>
                        2. <strong>Backward:</strong><br>
                        &nbsp;&nbsp;&nbsp;‚àÇL/‚àÇa = (a-y) = -0.12<br>
                        &nbsp;&nbsp;&nbsp;‚àÇa/‚àÇz = œÉ(z)(1-œÉ(z)) = 0.88 * 0.12 = 0.1056<br>
                        &nbsp;&nbsp;&nbsp;Œ¥ = ‚àÇL/‚àÇz = -0.12 * 0.1056 = -0.01267<br>
                        &nbsp;&nbsp;&nbsp;<strong>‚àÇL/‚àÇw = Œ¥ * x</strong> | <strong>‚àÇL/‚àÇb = Œ¥</strong>
                    </div>
                `
            },
            "regularization": {
                overview: `
                    <h3>Regularization Techniques</h3>
                    <table>
                        <tr>
                            <th>Method</th>
                            <th>How It Works</th>
                            <th>When to Use</th>
                        </tr>
                        <tr>
                            <td>L2 (Ridge)</td>
                            <td>Adds ŒªŒ£w¬≤ to loss</td>
                            <td>Keeps all features, reduces magnitude</td>
                        </tr>
                        <tr>
                            <td>L1 (Lasso)</td>
                            <td>Adds ŒªŒ£|w| to loss</td>
                            <td>Feature selection (zeros out weights)</td>
                        </tr>
                        <tr>
                            <td>Dropout</td>
                            <td>Randomly drops neurons (p=0.5 typical)</td>
                            <td><strong>Most effective for deep networks</strong></td>
                        </tr>
                        <tr>
                            <td>Early Stopping</td>
                            <td>Stop when validation loss increases</td>
                            <td>Prevents overfitting during training</td>
                        </tr>
                        <tr>
                            <td>Data Augmentation</td>
                            <td>Artificially expand dataset</td>
                            <td>Computer vision (rotations, flips, crops)</td>
                        </tr>
                    </table>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üéØ Best Practices</div>
                        <div class="box-content">
                            ‚Ä¢ Start with Dropout (0.5) for hidden layers<br>
                            ‚Ä¢ Add L2 if still overfitting (Œª=0.01, 0.001)<br>
                            ‚Ä¢ Always use Early Stopping<br>
                            ‚Ä¢ Data Augmentation for images
                        </div>
                    </div>
                `
            },
            "batch-norm": {
                overview: `
                    <h3>Batch Normalization</h3>
                    <p>Normalizes layer inputs to have mean=0 and variance=1, stabilizing and accelerating training.</p>
                    
                    <div class="callout tip">
                        <div class="callout-title">‚úÖ Benefits</div>
                        ‚Ä¢ <strong>Faster Training:</strong> Allows higher learning rates<br>
                        ‚Ä¢ <strong>Reduces Vanishing Gradients:</strong> Better gradient flow<br>
                        ‚Ä¢ <strong>Regularization Effect:</strong> Adds slight noise<br>
                        ‚Ä¢ <strong>Less Sensitive to Init:</strong> Reduces initialization impact
                    </div>
                `,
                math: `
                    <h3>The 4 Steps of Batch Normalization</h3>
                    <p>Calculated per mini-batch B = {x‚ÇÅ, ..., x‚Çò}:</p>
                    
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Mini-Batch Mean:</strong> Œº_B = (1/m) Œ£ x·µ¢</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Mini-Batch Variance:</strong> œÉ¬≤_B = (1/m) Œ£ (x·µ¢ - Œº_B)¬≤</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div><strong>Normalize:</strong> xÃÇ·µ¢ = (x·µ¢ - Œº_B) / ‚àö(œÉ¬≤_B + Œµ)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">04</div>
                        <div><strong>Scale and Shift:</strong> y·µ¢ = Œ≥ xÃÇ·µ¢ + Œ≤</div>
                    </div>

                    <div class="callout insight">
                        <div class="callout-title">üìù Paper & Pain: Why Œ≥ and Œ≤?</div>
                        If we only normalized to (0,1), we might restrict the representation power of the network. <br>
                        Œ≥ and Œ≤ allow the network to <strong>undo</strong> the normalization if that's optimal: <br>
                        If Œ≥ = ‚àö(œÉ¬≤) and Œ≤ = Œº, we get the original data back!
                    </div>
                `
            },
            "cv-intro": {
                overview: `
                    <h3>Why Computer Vision Needs Special Architectures</h3>
                    <p><strong>Problem:</strong> Images have huge dimensionality</p>
                    <ul>
                        <li>224√ó224 RGB image = 150,528 input features</li>
                        <li>Fully connected layer with 1000 neurons = 150M parameters!</li>
                        <li>Result: Overfitting, slow training, memory issues</li>
                    </ul>
                    
                    <h3>Solution: Convolutional Neural Networks</h3>
                    <ul>
                        <li><strong>Weight Sharing:</strong> Same filter applied everywhere (1000x fewer parameters)</li>
                        <li><strong>Local Connectivity:</strong> Neurons see small patches</li>
                        <li><strong>Translation Invariance:</strong> Detect cat anywhere in image</li>
                    </ul>
                `,
                concepts: `
                    <h3>Why CNNs Beat Fully Connected</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Parameter Efficiency:</strong> 1000√ó fewer parameters through weight sharing</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Translation Equivariance:</strong> Same object ‚Üí same activation regardless of position</div>
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üì∏ All Computer Vision Tasks</div>
                        <div class="box-content">
                            Image classification, object detection, segmentation, face recognition, OCR, medical imaging
                        </div>
                    </div>
                `
            },
            "pooling": {
                overview: `
                    <h3>Pooling Layers</h3>
                    <p>Pooling reduces spatial dimensions while retaining important information.</p>
                    
                    <table>
                        <tr>
                            <th>Type</th>
                            <th>Operation</th>
                            <th>Use Case</th>
                        </tr>
                        <tr>
                            <td>Max Pooling</td>
                            <td>Take maximum value</td>
                            <td><strong>Most common</strong> - preserves strong activations</td>
                        </tr>
                        <tr>
                            <td>Average Pooling</td>
                            <td>Take average</td>
                            <td>Smoother, less common (used in final layers)</td>
                        </tr>
                        <tr>
                            <td>Global Pooling</td>
                            <td>Pool entire feature map</td>
                            <td>Replace FC layers (reduces parameters)</td>
                        </tr>
                    </table>
                    
                    <div class="callout tip">
                        <div class="callout-title">‚úÖ Benefits</div>
                        ‚Ä¢ Reduces spatial size (faster computation)<br>
                        ‚Ä¢ Adds translation invariance<br>
                        ‚Ä¢ Prevents overfitting<br>
                        ‚Ä¢ Typical: 2√ó2 window, stride 2 (halves dimensions)
                    </div>
                `,
                concepts: `
                    <h3>Pooling Mechanics</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Downsampling:</strong> Reduces H√óW by pooling factor (typically 2√ó)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>No Learnable Parameters:</strong> Fixed operation (max/average)</div>
                    </div>
                    <div class="formula">
                        Example: 4√ó4 input ‚Üí 2√ó2 max pooling ‚Üí 2√ó2 output
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üéØ Standard CNN Component</div>
                        <div class="box-content">
                            Used after conv layers in AlexNet, VGG, and most classic CNNs to progressively reduce spatial dimensions
                        </div>
                    </div>
                `
            },
            "cnn-basics": {
                overview: `
                    <h3>CNN Architecture Pattern</h3>
                    <div class="formula">
                        Input ‚Üí [Conv ‚Üí ReLU ‚Üí Pool] √ó N ‚Üí Flatten ‚Üí FC ‚Üí Softmax
                    </div>
                    
                    <h3>Typical Layering Strategy</h3>
                    <ul>
                        <li><strong>Early Layers:</strong> Detect low-level features (edges, textures) - small filters (3√ó3)</li>
                        <li><strong>Middle Layers:</strong> Combine into patterns, parts - more filters, same size</li>
                        <li><strong>Deep Layers:</strong> High-level concepts (faces, objects) - many filters</li>
                        <li><strong>Final FC Layers:</strong> Classification based on learned features</li>
                    </ul>
                    
                    <div class="callout insight">
                        <div class="callout-title">üí° Filter Progression</div>
                        Layer 1: 32 filters (edges)<br>
                        Layer 2: 64 filters (textures)<br>
                        Layer 3: 128 filters (patterns)<br>
                        Layer 4: 256 filters (parts)<br>
                        Common pattern: double filters after each pooling
                    </div>
                `,
                concepts: `
                    <h3>Module Design Principles</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Spatial Reduction:</strong> Progressively downsample (224‚Üí112‚Üí56‚Üí28...)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Channel Expansion:</strong> Increase filters as spatial dims decrease</div>
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üéØ All Modern Vision Models</div>
                        <div class="box-content">
                            This pattern forms the backbone of ResNet, MobileNet, EfficientNet - fundamental CNN design
                        </div>
                    </div>
                `,
                math: `
                    <h3>1. The Golden Formula for Output Size</h3>
                    <p>Given Input (W), Filter Size (F), Padding (P), and Stride (S):</p>
                    <div class="formula" style="font-size: 1.2rem; text-align: center; margin: 20px 0;">
                        Output Size = ‚åä(W - F + 2P) / S‚åã + 1
                    </div>

                    <h3>2. Parameter Count Calculation</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Parameters PER Filter:</strong> (F √ó F √ó C_in) + 1 (bias)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Total Parameters:</strong> N_filters √ó ((F √ó F √ó C_in) + 1)</div>
                    </div>

                    <div class="callout insight">
                        <div class="callout-title">üìù Paper & Pain Calculation</div>
                        <strong>Input:</strong> 224x224x3 | <strong>Layer:</strong> 64 filters of 3x3 | <strong>Stride:</strong> 1 | <strong>Padding:</strong> 1<br>
                        1. <strong>Output Size:</strong> (224 - 3 + 2(1))/1 + 1 = 224 (Same Padding)<br>
                        2. <strong>Params:</strong> 64 * (3 * 3 * 3 + 1) = 64 * 28 = <strong>1,792 parameters</strong><br>
                        3. <strong>FLOPs:</strong> 224 * 224 * 1792 ‚âà <strong>90 Million operations</strong> per image!
                    </div>
                `
            },
            "viz-filters": {
                overview: `
                    <h3>What CNNs Learn</h3>
                    <p>CNN filters automatically learn hierarchical visual features:</p>
                    
                    <h3>Layer-by-Layer Visualization</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Layer 1:</strong> Edges and colors (horizontal, vertical, diagonal lines)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Layer 2:</strong> Textures and patterns (corners, curves, simple shapes)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div><strong>Layer 3:</strong> Object parts (eyes, wheels, windows)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">04</div>
                        <div><strong>Layer 4-5:</strong> Whole objects (faces, cars, animals)</div>
                    </div>
                `,
                concepts: `
                    <h3>Visualization Techniques</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Activation Maximization:</strong> Find input that maximizes filter response</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Grad-CAM:</strong> Highlight important regions for predictions</div>
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üîç Model Interpretability</div>
                        <div class="box-content">
                            Understanding what CNNs learn helps debug failures, build trust, and improve architecture design
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üé® Art & Style Transfer</div>
                        <div class="box-content">
                            Filter visualizations inspired neural style transfer (VGG features)
                        </div>
                    </div>
                `
            },
            "lenet": {
                overview: `
                    <h3>LeNet-5 (1998) - The Pioneer</h3>
                    <p>First successful CNN for digit recognition (MNIST). Introduced the Conv ‚Üí Pool ‚Üí Conv ‚Üí Pool pattern still used today.</p>
                    
                    <h3>Architecture</h3>
                    <div class="formula">
                        Input 32√ó32 ‚Üí Conv(6 filters, 5√ó5) ‚Üí AvgPool ‚Üí Conv(16 filters, 5√ó5) ‚Üí AvgPool ‚Üí FC(120) ‚Üí FC(84)‚Üí FC(10)
                    </div>
                    
                    <div class="callout insight">
                        <div class="callout-title">üèÜ Historical Impact</div>
                        ‚Ä¢ Used by US Postal Service for zip code recognition<br>
                        ‚Ä¢ Proved CNNs work for real-world tasks<br>
                        ‚Ä¢ Template for modern architectures
                    </div>
                `,
                concepts: `
                    <h3>Key Innovations</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Layered Architecture:</strong> Hierarchical feature extraction</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Shared Weights:</strong> Convolutional parameter sharing</div>
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">‚úâÔ∏è Handwriting Recognition</div>
                        <div class="box-content">
                            USPS mail sorting, check processing, form digitization
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üìö Educational Foundation</div>
                        <div class="box-content">
                            Perfect starting point for learning CNNs - simple enough to understand, complex enough to be useful
                        </div>
                    </div>
                `
            },
            "alexnet": {
                overview: `
                    <h3>AlexNet (2012) - The Deep Learning Revolution</h3>
                    <p>Won ImageNet 2012 by huge margin (15.3% vs 26.2% error), igniting the deep learning revolution.</p>
                    
                    <h3>Key Innovations</h3>
                    <ul>
                        <li><strong>ReLU Activation:</strong> Faster training than sigmoid/tanh</li>
                        <li><strong>Dropout:</strong> Prevents overfitting (p=0.5)</li>
                        <li><strong>Data Augmentation:</strong> Random crops/flips</li>
                        <li><strong>GPU Training:</strong> Used 2 GTX580 GPUs</li>
                        <li><strong>Deep:</strong> 8 layers (5 conv + 3 FC), 60M parameters</li>
                    </ul>
                    
                    <div class="callout tip">
                        <div class="callout-title">üí° Why So Important?</div>
                        First to show that deeper networks + more data + GPU compute = breakthrough performance
                    </div>
                `,
                concepts: `
                    <h3>Technical Contributions</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>ReLU:</strong> Solved vanishing gradients, enabled deeper networks</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Dropout:</strong> First major regularization for deep nets</div>
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üéØ ImageNet Challenge</div>
                        <div class="box-content">
                            Shattered records on 1000-class classification, proving deep learning superiority
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üöÄ Industry Catalyst</div>
                        <div class="box-content">
                            Sparked AI renaissance - Google, Facebook, Microsoft pivoted to deep learning after AlexNet
                        </div>
                    </div>
                `
            },
            "vgg": {
                overview: `
                    <h3>VGGNet (2014) - The Power of Depth</h3>
                    <p>VGG showed that depth matters - 16-19 layers using only small 3√ó3 filters.</p>
                    
                    <h3>Key Insight: Stacking Small Filters</h3>
                    <p>Two 3√ó3 conv layers = same receptive field as one 5√ó5, but:</p>
                    <ul>
                        <li><strong>Fewer Parameters:</strong> 2√ó(3¬≤) = 18 vs 5¬≤ = 25</li>
                        <li><strong>More Non-linearity:</strong> Two ReLUs instead of one</li>
                        <li><strong>Deeper Network:</strong> Better feature learning</li>
                    </ul>
                    
                    <div class="callout warning">
                        <div class="callout-title">‚ö†Ô∏è Limitation</div>
                        138M parameters (VGG-16) - very memory intensive for deployment
                    </div>
                `
            },
            "resnet": {
                overview: `
                    <h3>ResNet (2015) - Residual Connections</h3>
                    <p><strong>Problem:</strong> Very deep networks (>20 layers) had degradation - training accuracy got worse!</p>
                    
                    <h3>Solution: Skip Connections</h3>
                    <div class="formula">
                        Instead of learning H(x), learn residual F(x) = H(x) - x<br>
                        Output: y = F(x) + x  (shortcut connection)
                    </div>
                    
                    <h3>Why Skip Connections Work</h3>
                    <ul>
                        <li><strong>Gradient Flow:</strong> Gradients flow directly through shortcuts</li>
                        <li><strong>Identity Mapping:</strong> Easy to learn identity (just set F(x)=0)</li>
                        <li><strong>Feature Reuse:</strong> Earlier features directly available to later layers</li>
                    </ul>
                    
                    <div class="callout tip">
                        <div class="callout-title">üèÜ Impact</div>
                        ‚Ä¢ Enabled training of 152-layer networks (even 1000+ layers)<br>
                        ‚Ä¢ Won ImageNet 2015<br>
                        ‚Ä¢ Skip connections now used everywhere (U-Net, Transformers, etc.)
                    </div>
                `
            },
            "inception": {
                overview: `
                    <h3>Inception/GoogLeNet (2014) - Going Wider</h3>
                    <p>Instead of going deeper, Inception modules go wider - using multiple filter sizes in parallel.</p>
                    
                    <h3>Inception Module</h3>
                    <div class="formula">
                        Input ‚Üí [1√ó1 conv] ‚äï [3√ó3 conv] ‚äï [5√ó5 conv] ‚äï [3√ó3 pool] ‚Üí Concatenate
                    </div>
                    
                    <h3>Key Innovation: 1√ó1 Convolutions</h3>
                    <ul>
                        <li><strong>Dimensionality Reduction:</strong> Reduce channels before expensive 3√ó3, 5√ó5</li>
                        <li><strong>Non-linearity:</strong> Add extra ReLU</li>
                        <li><strong>Bottleneck Design:</strong> Reduces FLOPs by 10√ó</li>
                    </ul>
                    
                    <div class="callout insight">
                        <div class="callout-title">üí° Efficiency</div>
                        22 layers but only 5M parameters (27√ó less than AlexNet!)
                    </div>
                `
            },
            "mobilenet": {
                overview: `
                    <h3>MobileNet - CNNs for Mobile Devices</h3>
                    <p>Designed for mobile/embedded vision using depthwise separable convolutions.</p>
                    
                    <h3>Depthwise Separable Convolution</h3>
                    <div class="formula">
                        Standard Conv = Depthwise Conv + Pointwise (1√ó1) Conv
                    </div>
                    
                    <h3>Computation Reduction</h3>
                    <table>
                        <tr>
                            <th>Method</th>
                            <th>Parameters</th>
                            <th>FLOPs</th>
                        </tr>
                        <tr>
                            <td>Standard 3√ó3 Conv</td>
                            <td>3√ó3√óC_in√óC_out</td>
                            <td>High</td>
                        </tr>
                        <tr>
                            <td>Depthwise Separable</td>
                            <td>3√ó3√óC_in + C_in√óC_out</td>
                            <td><strong>8-9√ó less!</strong></td>
                        </tr>
                    </table>
                    
                    <div class="callout tip">
                        <div class="callout-title">‚úÖ Applications</div>
                        ‚Ä¢ Real-time mobile apps (camera filters, AR)<br>
                        ‚Ä¢ Edge devices (drones, IoT)<br>
                        ‚Ä¢ Latency-critical systems<br>
                        ‚Ä¢ Good accuracy with 10-20√ó speedup
                    </div>
                `
            },
            "transfer-learning": {
                overview: `
                    <h3>Transfer Learning - Don't Train from Scratch!</h3>
                    <p>Use pre-trained models (ImageNet) as feature extractors for your custom task.</p>
                    
                    <h3>Two Strategies</h3>
                    <table>
                        <tr>
                            <th>Approach</th>
                            <th>When to Use</th>
                            <th>How</th>
                        </tr>
                        <tr>
                            <td>Feature Extraction</td>
                            <td><strong>Small dataset</strong> (<10K images)</td>
                            <td>Freeze all layers, train only final FC layer</td>
                        </tr>
                        <tr>
                            <td>Fine-tuning</td>
                            <td><strong>Medium dataset</strong> (10K-100K)</td>
                            <td>Freeze early layers, train last few + FC</td>
                        </tr>
                        <tr>
                            <td>Full Training</td>
                            <td><strong>Large dataset</strong> (>1M images)</td>
                            <td>Use pre-trained as initialization, train all</td>
                        </tr>
                    </table>
                    
                    <div class="callout tip">
                        <div class="callout-title">üí° Best Practices</div>
                        ‚Ä¢ Use pre-trained models when dataset < 100K images<br>
                        ‚Ä¢ Start with low learning rate (1e-4) for fine-tuning<br>
                        ‚Ä¢ Popular backbones: ResNet50, EfficientNet, ViT
                    </div>
                `
            },
            "localization": {
                overview: `
                    <h3>Object Localization</h3>
                    <p>Predict both class and bounding box for a single object in image.</p>
                    
                    <h3>Multi-Task Loss</h3>
                    <div class="formula">
                        Total Loss = L_classification + Œª √ó L_bbox<br>
                        <br>
                        Where:<br>
                        L_classification = Cross-Entropy<br>
                        L_bbox = Smooth L1 or IoU loss<br>
                        Œª = balance term (typically 1-10)
                    </div>
                    
                    <h3>Bounding Box Representation</h3>
                    <ul>
                        <li><strong>Option 1:</strong> (x_min, y_min, x_max, y_max)</li>
                        <li><strong>Option 2:</strong> (x_center, y_center, width, height) ‚Üê Most common</li>
                    </ul>
                `
            },
            "rcnn": {
                overview: `
                    <h3>R-CNN Family Evolution</h3>
                    <table>
                        <tr>
                            <th>Model</th>
                            <th>Year</th>
                            <th>Speed (FPS)</th>
                            <th>Key Innovation</th>
                        </tr>
                        <tr>
                            <td>R-CNN</td>
                            <td>2014</td>
                            <td>0.05</td>
                            <td>Selective Search + CNN features</td>
                        </tr>
                        <tr>
                            <td>Fast R-CNN</td>
                            <td>2015</td>
                            <td>0.5</td>
                            <td>RoI Pooling (share conv features)</td>
                        </tr>
                        <tr>
                            <td>Faster R-CNN</td>
                            <td>2015</td>
                            <td>7</td>
                            <td>Region Proposal Network (RPN)</td>
                        </tr>
                        <tr>
                            <td>Mask R-CNN</td>
                            <td>2017</td>
                            <td>5</td>
                            <td>+ Instance Segmentation masks</td>
                        </tr>
                    </table>
                    
                    <div class="callout tip">
                        <div class="callout-title">üí° When to Use</div>
                        Faster R-CNN: Best accuracy for detection (not real-time)<br>
                        Mask R-CNN: Detection + instance segmentation
                    </div>
                `
            },
            "ssd": {
                overview: `
                    <h3>SSD (Single Shot MultiBox Detector)</h3>
                    <p>Balances speed and accuracy by predicting boxes at multiple scales.</p>
                    
                    <h3>Key Ideas</h3>
                    <ul>
                        <li><strong>Multi-Scale:</strong> Predictions from different layers (early = small objects, deep = large)</li>
                        <li><strong>Default Boxes (Anchors):</strong> Pre-defined boxes of various aspects ratios</li>
                        <li><strong>Single Pass:</strong> No separate region proposal step</li>
                    </ul>
                    
                    <div class="callout insight">
                        <div class="callout-title">üìä Performance</div>
                        SSD300: 59 FPS, 74.3% mAP<br>
                        SSD512: 22 FPS, 76.8% mAP<br>
                        <br>
                        Sweet spot between YOLO (faster) and Faster R-CNN (more accurate)
                    </div>
                `
            },
            "semantic-seg": {
                overview: `
                    <h3>Semantic Segmentation</h3>
                    <p>Classify every pixel in the image (pixel-wise classification).</p>
                    
                    <h3>Popular Architectures</h3>
                    <table>
                        <tr>
                            <th>Model</th>
                            <th>Key Feature</th>
                        </tr>
                        <tr>
                            <td>FCN</td>
                            <td>Fully Convolutional (no FC layers)</td>
                        </tr>
                        <tr>
                            <td>U-Net</td>
                            <td>Skip connections from encoder to decoder</td>
                        </tr>
                        <tr>
                            <td>DeepLab</td>
                            <td>Atrous (dilated) convolutions + ASPP</td>
                        </tr>
                    </table>
                    
                    <div class="formula">
                        U-Net Pattern:<br>
                        Input ‚Üí Encoder (downsample) ‚Üí Bottleneck ‚Üí Decoder (upsample) ‚Üí Pixel-wise Output<br>
                        With skip connections from encoder to decoder at each level
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üè• Medical Imaging</div>
                        <div class="box-content">Tumor segmentation, organ delineation, cell analysis</div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üöó Autonomous Driving</div>
                        <div class="box-content">Road segmentation, free space detection, drivable area</div>
                    </div>
                `
            },
            "instance-seg": {
                overview: `
                    <h3>Instance Segmentation</h3>
                    <p>Detect AND segment each individual object (combines object detection + semantic segmentation).</p>
                    
                    <h3>Difference from Semantic Segmentation</h3>
                    <ul>
                        <li><strong>Semantic:</strong> All "person" pixels get same label</li>
                        <li><strong>Instance:</strong> Person #1, Person #2, Person #3 (separate instances)</li>
                    </ul>
                    
                    <h3>Main Approach: Mask R-CNN</h3>
                    <div class="formula">
                        Faster R-CNN + Segmentation Branch<br>
                        <br>
                        For each RoI:<br>
                        1. Bounding box regression<br>
                        2. Class prediction<br>
                        3. <strong>Binary mask for the object</strong>
                    </div>
                `
            },
            "face-recog": {
                overview: `
                    <h3>Face Recognition with Siamese Networks</h3>
                    <p>Learn similarity between faces using metric learning instead of classification.</p>
                    
                    <h3>Triplet Loss Training</h3>
                    <div class="formula">
                        Loss = max(||f(A) - f(P)||¬≤ - ||f(A) - f(N)||¬≤ + margin, 0)<br>
                        <br>
                        Where:<br>
                        A = Anchor (reference face)<br>
                        P = Positive (same person)<br>
                        N = Negative (different person)<br>
                        margin = minimum separation (e.g., 0.2)
                    </div>
                    
                    <div class="callout tip">
                        <div class="callout-title">üí° One-Shot Learning</div>
                        After training, recognize new people with just 1-2 photos!<br>
                        No retraining needed - just compare embeddings.
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üì± Phone Unlock</div>
                        <div class="box-content">Face ID, biometric authentication</div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üîí Security</div>
                        <div class="box-content">Access control, surveillance, identity verification</div>
                    </div>
                `
            },
            "autoencoders": {
                overview: `
                    <h3>Autoencoders</h3>
                    <p>Unsupervised learning to compress data into latent representation and reconstruct it.</p>
                    
                    <h3>Architecture</h3>
                    <div class="formula">
                        Input ‚Üí Encoder ‚Üí Latent Code (bottleneck) ‚Üí Decoder ‚Üí Reconstruction<br>
                        <br>
                        Loss = ||Input - Reconstruction||¬≤ (MSE)
                    </div>
                    
                    <h3>Variants</h3>
                    <ul>
                        <li><strong>Vanilla:</strong> Basic autoencoder</li>
                        <li><strong>Denoising:</strong> Input corrupted, output clean (learns robust features)</li>
                        <li><strong>Variational (VAE):</strong> Probabilistic latent space (for generation)</li>
                        <li><strong>Sparse:</strong> Encourage sparse activations</li>
                    </ul>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üóúÔ∏è Compression</div>
                        <div class="box-content">Dimensionality reduction, data compression, feature extraction</div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üîç Anomaly Detection</div>
                        <div class="box-content">High reconstruction error = anomaly (fraud detection, defect detection)</div>
                    </div>
                `
            },
            "gans": {
                overview: `
                    <h3>GANs (Generative Adversarial Networks)</h3>
                    <p>Two networks compete: Generator creates fake data, Discriminator tries to detect fakes.</p>
                    
                    <h3>The GAN Game</h3>
                    <div class="formula">
                        Generator: Creates fake images from random noise<br>
                        Goal: Fool discriminator<br>
                        <br>
                        Discriminator: Classifies real vs fake<br>
                        Goal: Correctly identify fakes<br>
                        <br>
                        Minimax Loss:<br>
                        min_G max_D E[log D(x)] + E[log(1 - D(G(z)))]
                    </div>
                    
                    <div class="callout warning">
                        <div class="callout-title">‚ö†Ô∏è Training Challenges</div>
                        ‚Ä¢ Mode collapse (Generator produces limited variety)<br>
                        ‚Ä¢ Training instability (careful tuning needed)<br>
                        ‚Ä¢ Convergence issues<br>
                        ‚Ä¢ Solutions: Wasserstein GAN, Spectral Normalization, StyleGAN improvements
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üé® Image Generation</div>
                        <div class="box-content">
                            <strong>StyleGAN:</strong> Photorealistic faces, art generation<br>
                            <strong>DCGAN:</strong> Bedroom images, object generation
                        </div>
                    </div>
                `,
                math: `
                    <h3>The Minimax Game Objective</h3>
                    <p>The original GAN objective from Ian Goodfellow (2014) is a zero-sum game between Discriminator (D) and Generator (G).</p>
                    
                    <div class="formula" style="font-size: 1.1rem; padding: 20px;">
                        min_G max_D V(D, G) = E_x‚àºp_data[log D(x)] + E_z‚àºp_z[log(1 - D(G(z)))]
                    </div>

                    <h3>Paper & Pain: Finding the Optimal Discriminator</h3>
                    <p>For a fixed Generator, the optimal Discriminator D* is:</p>
                    <div class="formula">
                        D*(x) = p_data(x) / (p_data(x) + p_g(x))
                    </div>
                    
                    <div class="callout insight">
                        <div class="callout-title">üìù Theoretical Insight</div>
                        When the Discriminator is optimal, the Generator's task is essentially to minimize the <strong>Jensen-Shannon Divergence (JSD)</strong> between the data distribution and the model distribution. <br>
                        <strong>Problem:</strong> JSD is "flat" when distributions don't overlap, leading to vanishing gradients. This is why <strong>Wasserstein GAN (WGAN)</strong> was invented‚Äîusing Earth Mover's distance instead!
                    </div>

                    <h3>Generator Gradient Problem</h3>
                    <p>Early in training, D(G(z)) is near 0. The term log(1-D(G(z))) has a very small gradient. </p>
                    <div class="list-item">
                        <div class="list-num">üí°</div>
                        <div><strong>Heuristic Fix:</strong> Instead of minimizing log(1-D(G(z))), we maximize <strong>log D(G(z))</strong>. This provides much stronger gradients early on!</div>
                    </div>
                `
            },
            "diffusion": {
                overview: `
                    <h3>Diffusion Models</h3>
                    <p>Learn to reverse a gradual noising process, generating high-quality images.</p>
                    
                    <h3>How Diffusion Works</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Forward Process:</strong> Gradually add Gaussian noise over T steps (x‚ÇÄ ‚Üí x‚ÇÅ ‚Üí ... ‚Üí x_T = pure noise)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Reverse Process:</strong> Train neural network to denoise (x_T ‚Üí x_{T-1} ‚Üí ... ‚Üí x‚ÇÄ = clean image)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div><strong>Generation:</strong> Start from random noise, iteratively denoise T steps</div>
                    </div>
                    
                    <div class="callout tip">
                        <div class="callout-title">‚úÖ Advantages over GANs</div>
                        ‚Ä¢ More stable training (no adversarial dynamics)<br>
                        ‚Ä¢ Better sample quality and diversity<br>
                        ‚Ä¢ Mode coverage (no mode collapse)<br>
                        ‚Ä¢ Controllable generation (text-to-image)
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üñºÔ∏è Text-to-Image</div>
                        <div class="box-content">
                            <strong>Stable Diffusion:</strong> Open-source, runs on consumer GPUs<br>
                            <strong>DALL-E 2:</strong> OpenAI's photorealistic generator<br>
                            <strong>Midjourney:</strong> Artistic image generation
                        </div>
                    </div>
                `
            },
            "rnn": {
                overview: `
                    <h3>RNNs & LSTMs</h3>
                    <p>Process sequences by maintaining hidden state that captures past information.</p>
                    
                    <h3>The Vanishing Gradient Problem</h3>
                    <p><strong>Problem:</strong> Standard RNNs can't learn long-term dependencies (gradients vanish over many time steps)</p>
                    <p><strong>Solution:</strong> LSTM (Long Short-Term Memory) with gating mechanisms</p>
                    
                    <h3>LSTM Gates</h3>
                    <ul>
                        <li><strong>Forget Gate:</strong> What to remove from cell state</li>
                        <li><strong>Input Gate:</strong> What new information to add</li>
                        <li><strong>Output Gate:</strong> What to output as hidden state</li>
                    </ul>
                    
                    <div class="callout warning">
                        <div class="callout-title">‚ö†Ô∏è Limitation</div>
                        Sequential processing (can't parallelize) - Transformers solved this!
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üìù Text Generation</div>
                        <div class="box-content">Character-level generation, autocomplete (before Transformers)</div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üéµ Time Series</div>
                        <div class="box-content">Stock prediction, weather forecasting, music generation</div>
                    </div>
                `,
                math: `
                    <h3>RNN State Equations</h3>
                    <p>Standard RNN processes a sequence x‚ÇÅ, x‚ÇÇ, ..., x‚Çú using a recurring hidden state h‚Çú.</p>
                    
                    <div class="formula">
                        h‚Çú = tanh(W‚Çï‚Çïh‚Çú‚Çã‚ÇÅ + W‚Çì‚Çïx‚Çú + b‚Çï)<br>
                        y‚Çú = W‚Çï·µßh‚Çú + b·µß
                    </div>

                    <h3>Paper & Pain: The Vanishing Gradient Derivation</h3>
                    <p>Why do RNNs fail on long sequences? Let's check the gradient ‚àÇL/‚àÇh‚ÇÅ:</p>
                    <div class="formula">
                        ‚àÇL/‚àÇh‚ÇÅ = (‚àÇL/‚àÇh‚Çú) √ó (‚àÇh‚Çú/‚àÇh‚Çú‚Çã‚ÇÅ) √ó (‚àÇh‚Çú‚Çã‚ÇÅ/‚àÇh‚Çú‚Çã‚ÇÇ) √ó ... √ó (‚àÇh‚ÇÇ/‚àÇh‚ÇÅ)<br>
                        <br>
                        Where ‚àÇh‚±º/‚àÇh‚±º‚Çã‚ÇÅ = W‚Çï‚Çï·µÄ diag(tanh'(z‚±º))
                    </div>
                    <div class="callout warning">
                        <div class="callout-title">‚ö†Ô∏è The Power Effect</div>
                        If the largest eigenvalue of W‚Çï‚Çï < 1: Gradients <strong>shrink exponentially</strong> (0.9¬π‚Å∞‚Å∞ ‚âà 0.00002).<br>
                        If > 1: Gradients <strong>explode</strong>.<br>
                        <strong>LSTM Solution:</strong> The "Constant Error Carousel" (CEC) ensures gradients flow via the cell state without multiplication.
                    </div>

                    <h3>LSTM Gating Math</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div>Forget Gate: f‚Çú = œÉ(W_f[h‚Çú‚Çã‚ÇÅ, x‚Çú] + b_f)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div>Input Gate: i‚Çú = œÉ(W_i[h‚Çú‚Çã‚ÇÅ, x‚Çú] + b_i)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div>Cell State Update: c‚Çú = f‚Çúc‚Çú‚Çã‚ÇÅ + i‚Çútanh(W_c[h‚Çú‚Çã‚ÇÅ, x‚Çú] + b_c)</div>
                    </div>
                `
            },
            "bert": {
                overview: `
                    <h3>BERT (Bidirectional Encoder Representations from Transformers)</h3>
                    <p>Pre-trained encoder-only Transformer for understanding language (not generation).</p>
                    
                    <h3>Key Innovation: Bidirectional Context</h3>
                    <p>Unlike GPT (left-to-right), BERT sees both left AND right context simultaneously.</p>
                    
                    <h3>Pre-training Tasks</h3>
                    <ul>
                        <li><strong>Masked Language Modeling:</strong> Mask 15% of tokens, predict them (e.g., "The cat [MASK] on the mat" ‚Üí predict "sat")</li>
                        <li><strong>Next Sentence Prediction:</strong> Predict if sentence B follows A</li>
                    </ul>
                    
                    <div class="callout tip">
                        <div class="callout-title">üí° Fine-tuning BERT</div>
                        1. Start with pre-trained BERT (trained on billions of words)<br>
                        2. Add task-specific head (classification, QA, NER)<br>
                        3. Fine-tune on your dataset (10K-100K examples)<br>
                        4. Achieves SOTA with minimal data!
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üîç Search & QA</div>
                        <div class="box-content">
                            <strong>Google Search:</strong> Uses BERT for understanding queries<br>
                            Question answering systems, document retrieval
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üìä Text Classification</div>
                        <div class="box-content">Sentiment analysis, topic classification, spam detection</div>
                    </div>
                `
            },
            "gpt": {
                overview: `
                    <h3>GPT (Generative Pre-trained Transformer)</h3>
                    <p>Decoder-only Transformer trained to predict next token (autoregressive language modeling).</p>
                    
                    <h3>GPT Evolution</h3>
                    <table>
                        <tr>
                            <th>Model</th>
                            <th>Params</th>
                            <th>Training Data</th>
                            <th>Capability</th>
                        </tr>
                        <tr>
                            <td>GPT-1</td>
                            <td>117M</td>
                            <td>BooksCorpus</td>
                            <td>Basic text generation</td>
                        </tr>
                        <tr>
                            <td>GPT-2</td>
                            <td>1.5B</td>
                            <td>WebText (40GB)</td>
                            <td>Coherent paragraphs</td>
                        </tr>
                        <tr>
                            <td>GPT-3</td>
                            <td>175B</td>
                            <td>570GB text</td>
                            <td>Few-shot learning</td>
                        </tr>
                        <tr>
                            <td>GPT-4</td>
                            <td>~1.8T</td>
                            <td>Multi-modal</td>
                            <td>Reasoning, coding, images</td>
                        </tr>
                    </table>
                    
                    <div class="callout insight">
                        <div class="callout-title">üöÄ Emergent Abilities</div>
                        As models scale, new capabilities emerge:<br>
                        ‚Ä¢ In-context learning (learn from prompts)<br>
                        ‚Ä¢ Chain-of-thought reasoning<br>
                        ‚Ä¢ Code generation<br>
                        ‚Ä¢ Multi-step problem solving
                    </div>
                `,
                applications: `
                    <div class="info-box">
                        <div class="box-title">üí¨ ChatGPT & Assistants</div>
                        <div class="box-content">
                            Conversational AI, customer support, tutoring, brainstorming
                        </div>
                    </div>
                    <div class="info-box">
                        <div class="box-title">üíª Code Generation</div>
                        <div class="box-content">
                            GitHub Copilot, code completion, bug fixing, documentation
                        </div>
                    </div>
                `
            },
            "vit": {
                overview: `
                    <h3>Vision Transformer (ViT)</h3>
                    <p>Apply Transformer architecture directly to images by treating them as sequences of patches.</p>
                    
                    <h3>How ViT Works</h3>
                    <div class="list-item">
                        <div class="list-num">01</div>
                        <div><strong>Patchify:</strong> Split 224√ó224 image into 16√ó16 patches (14√ó14 = 196 patches)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">02</div>
                        <div><strong>Linear Projection:</strong> Flatten each patch ‚Üí linear embedding (like word embeddings)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">03</div>
                        <div><strong>Positional Encoding:</strong> Add position information</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">04</div>
                        <div><strong>Transformer Encoder:</strong> Standard Transformer (self-attention, FFN)</div>
                    </div>
                    <div class="list-item">
                        <div class="list-num">05</div>
                        <div><strong>Classification:</strong> Use [CLS] token for final prediction</div>
                    </div>
                    
                    <div class="callout tip">
                        <div class="callout-title">üí° When ViT Shines</div>
                        ‚Ä¢ <strong>Large Datasets:</strong> Needs 10M+ images (or pre-training on ImageNet-21K)<br>
                        ‚Ä¢ <strong>Transfer Learning:</strong> Pre-trained ViT beats CNNs on many tasks<br>
                        ‚Ä¢ <strong>Long-Range Dependencies:</strong> Global attention vs CNN's local receptive field
                    </div>
                `
            }
        };

        function createModuleHTML(module) {
            const content = MODULE_CONTENT[module.id] || {};

            return `
                <div class="module" id="${module.id}-module">
                    <button class="btn-back" onclick="switchTo('dashboard')">‚Üê Back to Dashboard</button>
                    <header>
                        <h1>${module.icon} ${module.title}</h1>
                        <p class="subtitle">${module.description}</p>
                    </header>

                    <div class="tabs">
                        <button class="tab-btn active" onclick="switchTab(event, '${module.id}-overview')">Overview</button>
                        <button class="tab-btn" onclick="switchTab(event, '${module.id}-concepts')">Key Concepts</button>
                        <button class="tab-btn" onclick="switchTab(event, '${module.id}-visualization')">üìä Visualization</button>
                        <button class="tab-btn" onclick="switchTab(event, '${module.id}-math')">Math</button>
                        <button class="tab-btn" onclick="switchTab(event, '${module.id}-applications')">Applications</button>
                        <button class="tab-btn" onclick="switchTab(event, '${module.id}-summary')">Summary</button>
                    </div>

                    <div id="${module.id}-overview" class="tab active">
                        <div class="section">
                            <h2>üìñ Overview</h2>
                            ${content.overview || `
                                <p>Complete coverage of ${module.title.toLowerCase()}. Learn the fundamentals, mathematics, real-world applications, and implementation details.</p>
                                <div class="info-box">
                                    <div class="box-title">Learning Objectives</div>
                                    <div class="box-content">
                                        ‚úì Understand core concepts and theory<br>
                                        ‚úì Master mathematical foundations<br>
                                        ‚úì Learn practical applications<br>
                                        ‚úì Implement and experiment
                                    </div>
                                </div>
                            `}
                        </div>
                    </div>

                    <div id="${module.id}-concepts" class="tab">
                        <div class="section">
                            <h2>üéØ Key Concepts</h2>
                            ${content.concepts || `
                                <p>Fundamental concepts and building blocks for ${module.title.toLowerCase()}.</p>
                                <div class="callout insight">
                                    <div class="callout-title">üí° Main Ideas</div>
                                    This section covers the core ideas you need to understand before diving into mathematics.
                                </div>
                            `}
                        </div>
                    </div>

                    <div id="${module.id}-visualization" class="tab">
                        <div class="section">
                            <h2>üìä Interactive Visualization</h2>
                            <p>Visual representation to help understand ${module.title.toLowerCase()} concepts intuitively.</p>
                            <div id="${module.id}-viz" class="viz-container">
                                <canvas id="${module.id}-canvas" width="800" height="400" style="border: 1px solid rgba(0, 212, 255, 0.3); border-radius: 8px; background: rgba(0, 212, 255, 0.02);"></canvas>
                            </div>
                            <div class="viz-controls">
                                <button onclick="drawVisualization('${module.id}')" class="btn-viz">üîÑ Refresh Visualization</button>
                                <button onclick="toggleVizAnimation('${module.id}')" class="btn-viz">‚ñ∂Ô∏è Animate</button>
                                <button onclick="downloadViz('${module.id}')" class="btn-viz">‚¨áÔ∏è Save Image</button>
                            </div>
                        </div>
                    </div>

                    <div id="${module.id}-math" class="tab">
                        <div class="section">
                            <h2>üìê Mathematical Foundation</h2>
                            ${content.math || `
                                <p>Rigorous mathematical treatment of ${module.title.toLowerCase()}.</p>
                                <div class="formula">
                                    Mathematical formulas and derivations go here
                                </div>
                            `}
                        </div>
                    </div>

                    <div id="${module.id}-applications" class="tab">
                        <div class="section">
                            <h2>üåç Real-World Applications</h2>
                            ${content.applications || `
                                <p>How ${module.title.toLowerCase()} is used in practice across different industries.</p>
                                <div class="info-box">
                                    <div class="box-title">Use Cases</div>
                                    <div class="box-content">
                                        Common applications and practical examples
                                    </div>
                                </div>
                            `}
                        </div>
                    </div>

                    <div id="${module.id}-summary" class="tab">
                        <div class="section">
                            <h2>‚úÖ Summary</h2>
                            <div class="info-box">
                                <div class="box-title">Key Takeaways</div>
                                <div class="box-content">
                                    ‚úì Essential concepts covered<br>
                                    ‚úì Mathematical foundations understood<br>
                                    ‚úì Real-world applications identified<br>
                                    ‚úì Ready for implementation
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            `;
        }

        function initDashboard() {
            const grid = document.getElementById("modulesGrid");
            const container = document.getElementById("modulesContainer");

            modules.forEach(module => {
                const card = document.createElement("div");
                card.className = "card";
                card.style.borderColor = module.color;
                card.onclick = () => switchTo(module.id + "-module");
                card.innerHTML = `
                    <div class="card-icon">${module.icon}</div>
                    <h3>${module.title}</h3>
                    <p>${module.description}</p>
                    <span class="category-label">${module.category}</span>
                `;
                grid.appendChild(card);

                const moduleHTML = createModuleHTML(module);
                container.innerHTML += moduleHTML;
            });
        }

        function switchTo(target) {
            document.querySelectorAll('.dashboard, .module').forEach(el => {
                el.classList.remove('active');
            });
            const elem = document.getElementById(target);
            if (elem) elem.classList.add('active');
        }

        function switchTab(e, tabId) {
            const module = e.target.closest('.module');
            if (!module) return;

            module.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
            module.querySelectorAll('.tab-btn').forEach(b => b.classList.remove('active'));

            const tab = document.getElementById(tabId);
            if (tab) tab.classList.add('active');
            e.target.classList.add('active');

            // Trigger visualization when tabs are clicked
            setTimeout(() => {
                const moduleId = tabId.split('-')[0];
                if (tabId.includes('-concepts')) {
                    drawConceptsVisualization(moduleId);
                } else if (tabId.includes('-visualization')) {
                    drawConceptsVisualization(moduleId);
                } else if (tabId.includes('-math')) {
                    drawMathVisualization(moduleId);
                } else if (tabId.includes('-applications')) {
                    drawApplicationVisualization(moduleId);
                }
            }, 150);
        }

        // Visualization Functions - Concepts Tab
        function drawConceptsVisualization(moduleId) {
            const canvas = document.getElementById(moduleId + '-canvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            ctx.fillStyle = '#0f1419';
            ctx.fillRect(0, 0, canvas.width, canvas.height);

            const vizMap = {
                'nn-basics': drawNeuronAnimation,
                'perceptron': drawDecisionBoundary,
                'mlp': drawNetworkGraph,
                'activation': drawActivationFunctions,
                'weight-init': drawWeightDistribution,
                'loss': drawLossLandscape,
                'optimizers': drawConvergencePaths,
                'backprop': drawGradientFlow,
                'regularization': drawOverfitComparison,
                'batch-norm': drawBatchNormalization,
                'cv-intro': drawImageMatrix,
                'conv-layer': drawConvolutionAnimation,
                'pooling': drawPoolingDemo,
                'cnn-basics': drawCNNArchitecture,
                'viz-filters': drawLearnedFilters,
                'lenet': drawLeNetArchitecture,
                'alexnet': drawAlexNetArchitecture,
                'vgg': drawVGGArchitecture,
                'resnet': drawResNetArchitecture,
                'inception': drawInceptionModule,
                'mobilenet': drawMobileNetArchitecture,
                'transfer-learning': drawTransferLearning,
                'localization': drawBoundingBoxes,
                'rcnn': drawRCNNPipeline,
                'yolo': drawYOLOGrid,
                'ssd': drawSSDDetector,
                'semantic-seg': drawSemanticSegmentation,
                'instance-seg': drawInstanceSegmentation,
                'face-recog': drawFaceEmbeddings,
                'autoencoders': drawAutoencoderArchitecture,
                'gans': drawGANsGame,
                'diffusion': drawDiffusionProcess,
                'rnn': drawRNNUnrolled,
                'transformers': drawAttentionMatrix,
                'bert': drawBERTProcess,
                'gpt': drawGPTGeneration,
                'vit': drawVisionTransformer
            };

            if (vizMap[moduleId]) {
                vizMap[moduleId](ctx, canvas);
            } else {
                drawDefaultVisualization(ctx, canvas);
            }
        }

        // Default Visualization
        function drawDefaultVisualization(ctx, canvas) {
            const centerX = canvas.width / 2;
            const centerY = canvas.height / 2;

            ctx.fillStyle = 'rgba(0, 212, 255, 0.2)';
            ctx.fillRect(centerX - 120, centerY - 60, 240, 120);
            ctx.strokeStyle = '#00d4ff';
            ctx.lineWidth = 2;
            ctx.strokeRect(centerX - 120, centerY - 60, 240, 120);

            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 18px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('üìä Interactive Visualization', centerX, centerY - 20);
            ctx.font = '13px Arial';
            ctx.fillText('Custom visualization for this topic', centerX, centerY + 20);
            ctx.font = '11px Arial';
            ctx.fillStyle = '#00ff88';
            ctx.fillText('Click Refresh to render', centerX, centerY + 45);
        }

        // Default Math Visualization
        function drawDefaultMathVisualization(ctx, canvas) {
            const centerX = canvas.width / 2;
            const centerY = canvas.height / 2;

            ctx.fillStyle = 'rgba(255, 107, 53, 0.2)';
            ctx.fillRect(centerX - 120, centerY - 60, 240, 120);
            ctx.strokeStyle = '#ff6b35';
            ctx.lineWidth = 2;
            ctx.strokeRect(centerX - 120, centerY - 60, 240, 120);

            ctx.fillStyle = '#ff6b35';
            ctx.font = 'bold 18px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('üìê Mathematical Formulas', centerX, centerY - 20);
            ctx.font = '13px Arial';
            ctx.fillText('Visual equation derivations', centerX, centerY + 20);
            ctx.font = '11px Arial';
            ctx.fillStyle = '#00ff88';
            ctx.fillText('Click Visualize to render', centerX, centerY + 45);
        }

        // Default Application Visualization
        function drawDefaultApplicationVisualization(ctx, canvas) {
            const centerX = canvas.width / 2;
            const centerY = canvas.height / 2;

            ctx.fillStyle = 'rgba(0, 255, 136, 0.2)';
            ctx.fillRect(centerX - 120, centerY - 60, 240, 120);
            ctx.strokeStyle = '#00ff88';
            ctx.lineWidth = 2;
            ctx.strokeRect(centerX - 120, centerY - 60, 240, 120);

            ctx.fillStyle = '#00ff88';
            ctx.font = 'bold 18px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('üåç Real-World Applications', centerX, centerY - 20);
            ctx.font = '13px Arial';
            ctx.fillText('Practical use cases and examples', centerX, centerY + 20);
            ctx.font = '11px Arial';
            ctx.fillStyle = '#ffa500';
            ctx.fillText('Click Show Applications to render', centerX, centerY + 45);
        }

        // Activation Functions Visualization
        function drawActivationFunctions(ctx, canvas) {
            const width = canvas.width;
            const height = canvas.height;
            const centerX = width / 2;
            const centerY = height / 2;
            const scale = 40;

            // Draw grid
            ctx.strokeStyle = 'rgba(0, 212, 255, 0.1)';
            ctx.lineWidth = 1;
            for (let i = -5; i <= 5; i += 1) {
                const x = centerX + i * scale;
                ctx.beginPath();
                ctx.moveTo(x, centerY - 5 * scale);
                ctx.lineTo(x, centerY + 5 * scale);
                ctx.stroke();
            }

            // Draw axes
            ctx.strokeStyle = '#00d4ff';
            ctx.lineWidth = 2;
            ctx.beginPath();
            ctx.moveTo(centerX - 6 * scale, centerY);
            ctx.lineTo(centerX + 6 * scale, centerY);
            ctx.stroke();
            ctx.beginPath();
            ctx.moveTo(centerX, centerY - 6 * scale);
            ctx.lineTo(centerX, centerY + 6 * scale);
            ctx.stroke();

            // Draw activation functions
            const functions = [
                { name: 'ReLU', color: '#ff6b35', fn: x => Math.max(0, x) },
                { name: 'Sigmoid', color: '#00ff88', fn: x => 1 / (1 + Math.exp(-x)) },
                { name: 'Tanh', color: '#ffa500', fn: x => Math.tanh(x) }
            ];

            functions.forEach(func => {
                ctx.strokeStyle = func.color;
                ctx.lineWidth = 2;
                ctx.beginPath();
                for (let x = -5; x <= 5; x += 0.1) {
                    const y = func.fn(x);
                    const canvasX = centerX + x * scale;
                    const canvasY = centerY - y * scale;
                    if (x === -5) ctx.moveTo(canvasX, canvasY);
                    else ctx.lineTo(canvasX, canvasY);
                }
                ctx.stroke();
            });

            // Legend
            ctx.font = 'bold 12px Arial';
            functions.forEach((func, i) => {
                ctx.fillStyle = func.color;
                ctx.fillRect(10, 10 + i * 20, 10, 10);
                ctx.fillStyle = '#e4e6eb';
                ctx.fillText(func.name, 25, 19 + i * 20);
            });
        }

        // Neural Network Graph
        function drawNetworkGraph(ctx, canvas) {
            const layers = [2, 3, 3, 1];
            const width = canvas.width;
            const height = canvas.height;
            const layerWidth = width / (layers.length + 1);

            ctx.fillStyle = 'rgba(0, 212, 255, 0.05)';
            ctx.fillRect(0, 0, width, height);

            // Draw neurons and connections
            const neuronPositions = [];

            layers.forEach((numNeurons, layerIdx) => {
                const x = (layerIdx + 1) * layerWidth;
                const positions = [];

                for (let i = 0; i < numNeurons; i++) {
                    const y = height / (numNeurons + 1) * (i + 1);
                    positions.push({ x, y });

                    // Draw connections to next layer
                    if (layerIdx < layers.length - 1) {
                        const nextLayerPositions = [];
                        const nextX = (layerIdx + 2) * layerWidth;
                        for (let j = 0; j < layers[layerIdx + 1]; j++) {
                            const nextY = height / (layers[layerIdx + 1] + 1) * (j + 1);
                            nextLayerPositions.push({ x: nextX, y: nextY });
                        }

                        nextLayerPositions.forEach(next => {
                            ctx.strokeStyle = 'rgba(0, 212, 255, 0.2)';
                            ctx.lineWidth = 1;
                            ctx.beginPath();
                            ctx.moveTo(x, y);
                            ctx.lineTo(next.x, next.y);
                            ctx.stroke();
                        });
                    }
                }

                // Draw neurons
                positions.forEach(pos => {
                    ctx.fillStyle = '#00d4ff';
                    ctx.beginPath();
                    ctx.arc(pos.x, pos.y, 8, 0, Math.PI * 2);
                    ctx.fill();
                });

                neuronPositions.push(positions);
            });

            // Labels
            ctx.fillStyle = '#e4e6eb';
            ctx.font = 'bold 12px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Input', layerWidth, height - 10);
            ctx.fillText('Hidden 1', layerWidth * 2, height - 10);
            ctx.fillText('Hidden 2', layerWidth * 3, height - 10);
            ctx.fillText('Output', layerWidth * 4, height - 10);
        }

        // Convolution Animation
        function drawConvolutionAnimation(ctx, canvas) {
            const width = canvas.width;
            const height = canvas.height;

            // Draw input image
            ctx.fillStyle = 'rgba(0, 212, 255, 0.1)';
            ctx.fillRect(20, 20, 150, 150);
            ctx.strokeStyle = '#00d4ff';
            ctx.lineWidth = 2;
            ctx.strokeRect(20, 20, 150, 150);

            // Draw filter
            ctx.fillStyle = 'rgba(255, 107, 53, 0.1)';
            const filterPos = 60 + Math.sin(Date.now() / 1000) * 40;
            ctx.fillRect(filterPos, 60, 60, 60);
            ctx.strokeStyle = '#ff6b35';
            ctx.lineWidth = 3;
            ctx.strokeRect(filterPos, 60, 60, 60);

            // Draw output
            ctx.fillStyle = 'rgba(0, 255, 136, 0.1)';
            ctx.fillRect(width - 170, 20, 150, 150);
            ctx.strokeStyle = '#00ff88';
            ctx.lineWidth = 2;
            ctx.strokeRect(width - 170, 20, 150, 150);

            // Draw feature map
            for (let i = 0; i < 5; i++) {
                for (let j = 0; j < 5; j++) {
                    const intensity = Math.random() * 100;
                    ctx.fillStyle = `rgba(0, 212, 255, ${intensity / 100})`;
                    ctx.fillRect(width - 160 + i * 25, 30 + j * 25, 20, 20);
                }
            }

            // Labels
            ctx.fillStyle = '#e4e6eb';
            ctx.font = 'bold 12px Arial';
            ctx.textAlign = 'left';
            ctx.fillText('Input Image', 20, 190);
            ctx.fillText('Filter', filterPos, 140);
            ctx.fillText('Feature Map', width - 170, 190);
        }

        // Loss Landscape
        function drawLossLandscape(ctx, canvas) {
            const width = canvas.width;
            const height = canvas.height;

            for (let x = 0; x < width; x += 20) {
                for (let y = 0; y < height; y += 20) {
                    const nx = (x - width / 2) / (width / 4);
                    const ny = (y - height / 2) / (height / 4);
                    const loss = nx * nx + ny * ny;
                    const intensity = Math.min(255, loss * 50);
                    ctx.fillStyle = `rgb(${intensity}, ${100}, ${255 - intensity})`;
                    ctx.fillRect(x, y, 20, 20);
                }
            }

            // Draw descent path
            ctx.strokeStyle = '#00ff88';
            ctx.lineWidth = 2;
            ctx.beginPath();
            const startX = width / 2 + 80;
            const startY = height / 2 + 80;
            ctx.moveTo(startX, startY);

            for (let i = 0; i < 20; i++) {
                const angle = Math.atan2(startY - height / 2, startX - width / 2);
                const newX = startX - Math.cos(angle) * 15;
                const newY = startY - Math.sin(angle) * 15;
                ctx.lineTo(newX, newY);
            }
            ctx.stroke();

            // Minimum point
            ctx.fillStyle = '#00ff88';
            ctx.beginPath();
            ctx.arc(width / 2, height / 2, 8, 0, Math.PI * 2);
            ctx.fill();
        }

        // YOLO Grid
        function drawYOLOGrid(ctx, canvas) {
            const width = canvas.width;
            const height = canvas.height;
            const gridSize = 7;
            const cellWidth = width / gridSize;
            const cellHeight = height / gridSize;

            // Draw grid
            ctx.strokeStyle = 'rgba(0, 212, 255, 0.3)';
            ctx.lineWidth = 1;
            for (let i = 0; i <= gridSize; i++) {
                ctx.beginPath();
                ctx.moveTo(i * cellWidth, 0);
                ctx.lineTo(i * cellWidth, height);
                ctx.stroke();

                ctx.beginPath();
                ctx.moveTo(0, i * cellHeight);
                ctx.lineTo(width, i * cellHeight);
                ctx.stroke();
            }

            // Draw detected objects
            const detections = [
                { x: 2, y: 2, w: 2, h: 2, conf: 0.95 },
                { x: 4, y: 5, w: 1.5, h: 1.5, conf: 0.87 }
            ];

            detections.forEach(det => {
                ctx.fillStyle = `rgba(255, 107, 53, ${det.conf * 0.5})`;
                ctx.fillRect(det.x * cellWidth, det.y * cellHeight, det.w * cellWidth, det.h * cellHeight);
                ctx.strokeStyle = '#ff6b35';
                ctx.lineWidth = 2;
                ctx.strokeRect(det.x * cellWidth, det.y * cellHeight, det.w * cellWidth, det.h * cellHeight);

                ctx.fillStyle = '#ff6b35';
                ctx.font = 'bold 12px Arial';
                ctx.fillText((det.conf * 100).toFixed(0) + '%', det.x * cellWidth + 5, det.y * cellHeight + 15);
            });
        }

        // Attention Matrix
        function drawAttentionMatrix(ctx, canvas) {
            const size = 8;
            const cellSize = Math.min(canvas.width, canvas.height) / size;

            for (let i = 0; i < size; i++) {
                for (let j = 0; j < size; j++) {
                    const distance = Math.abs(i - j);
                    const attention = Math.exp(-distance / 2);
                    const intensity = Math.floor(attention * 255);
                    ctx.fillStyle = `rgb(${intensity}, 100, ${200 - intensity})`;
                    ctx.fillRect(i * cellSize, j * cellSize, cellSize, cellSize);
                }
            }

            // Add labels
            ctx.fillStyle = '#e4e6eb';
            ctx.font = '10px Arial';
            ctx.textAlign = 'center';
            for (let i = 0; i < size; i++) {
                ctx.fillText('w' + i, i * cellSize + cellSize / 2, canvas.height - 5);
            }
        }

        // Math Visualization
        function drawMathVisualization(moduleId) {
            const canvas = document.getElementById(moduleId + '-math-canvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            ctx.fillStyle = '#0f1419';
            ctx.fillRect(0, 0, canvas.width, canvas.height);

            const mathVizMap = {
                'nn-basics': () => drawNNMath(ctx, canvas),
                'activation': () => drawActivationDerivatives(ctx, canvas),
                'loss': () => drawLossComparison(ctx, canvas),
                'optimizers': () => drawOptimizerSteps(ctx, canvas),
                'backprop': () => drawChainRule(ctx, canvas),
                'conv-layer': () => drawConvolutionMath(ctx, canvas),
                'pooling': () => drawPoolingMath(ctx, canvas),
                'regularization': () => drawRegularizationMath(ctx, canvas),
                'transformers': () => drawAttentionMath(ctx, canvas),
                'rnn': () => drawRNNMath(ctx, canvas)
            };

            if (mathVizMap[moduleId]) {
                mathVizMap[moduleId]();
            } else {
                drawDefaultMathVisualization(ctx, canvas);
            }
        }

        // Application Visualization
        function drawApplicationVisualization(moduleId) {
            const canvas = document.getElementById(moduleId + '-app-canvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            ctx.fillStyle = '#0f1419';
            ctx.fillRect(0, 0, canvas.width, canvas.height);

            const appVizMap = {
                'nn-basics': () => drawNNApplications(ctx, canvas),
                'cnn-basics': () => drawCNNApplications(ctx, canvas),
                'conv-layer': () => drawConvolutionApplications(ctx, canvas),
                'yolo': () => drawYOLOApplications(ctx, canvas),
                'semantic-seg': () => drawSegmentationApplications(ctx, canvas),
                'instance-seg': () => drawInstanceSegmentationApps(ctx, canvas),
                'face-recog': () => drawFaceRecognitionApps(ctx, canvas),
                'transformers': () => drawTransformerApps(ctx, canvas),
                'bert': () => drawBERTApplications(ctx, canvas),
                'gpt': () => drawGPTApplications(ctx, canvas),
                'gans': () => drawGANApplications(ctx, canvas),
                'diffusion': () => drawDiffusionApplications(ctx, canvas)
            };

            if (appVizMap[moduleId]) {
                appVizMap[moduleId]();
            } else {
                drawDefaultApplicationVisualization(ctx, canvas);
            }
        }

        // Math visualization helper functions
        function drawNNMath(ctx, canvas) {
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 18px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Forward Pass: y = œÉ(Wx + b)', canvas.width / 2, 50);
            ctx.font = '14px Arial';
            ctx.fillStyle = '#00ff88';
            ctx.fillText('Linear combination + Non-linearity', canvas.width / 2, 100);
            ctx.fillStyle = '#ffa500';
            ctx.fillText('W: weights, b: bias, œÉ: activation', canvas.width / 2, 150);
        }

        function drawActivationDerivatives(ctx, canvas) {
            const width = canvas.width;
            const height = canvas.height;
            const centerX = width / 2;
            const centerY = height / 2;
            const scale = 40;

            ctx.strokeStyle = 'rgba(0, 212, 255, 0.2)';
            ctx.lineWidth = 1;
            for (let i = -5; i <= 5; i += 1) {
                ctx.beginPath();
                ctx.moveTo(centerX + i * scale, centerY - 5 * scale);
                ctx.lineTo(centerX + i * scale, centerY + 5 * scale);
                ctx.stroke();
            }

            ctx.strokeStyle = '#00ff88';
            ctx.lineWidth = 3;
            ctx.beginPath();
            for (let x = -5; x <= 5; x += 0.1) {
                const y = 1 / (1 + Math.exp(-x)) * (1 - 1 / (1 + Math.exp(-x)));
                const canvasX = centerX + x * scale;
                const canvasY = centerY - y * scale * 10;
                if (x === -5) ctx.moveTo(canvasX, canvasY);
                else ctx.lineTo(canvasX, canvasY);
            }
            ctx.stroke();

            ctx.fillStyle = '#00ff88';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText("Sigmoid Derivative: œÉ'(x) = œÉ(x)(1-œÉ(x))", canvas.width / 2, 30);
        }

        function drawLossComparison(ctx, canvas) {
            const width = canvas.width;
            const height = canvas.height;

            // MSE
            ctx.fillStyle = 'rgba(0, 212, 255, 0.2)';
            ctx.fillRect(20, 60, width / 2 - 30, height - 100);
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 14px Arial';
            ctx.fillText('MSE Loss', width / 4, 45);
            ctx.font = '12px Arial';
            ctx.fillText('L = (1/n)Œ£(y-≈∑)¬≤', width / 4, 90);
            ctx.fillText('Regression', width / 4, 115);

            // Cross-Entropy
            ctx.fillStyle = 'rgba(255, 107, 53, 0.2)';
            ctx.fillRect(width / 2 + 10, 60, width / 2 - 30, height - 100);
            ctx.fillStyle = '#ff6b35';
            ctx.font = 'bold 14px Arial';
            ctx.fillText('Cross-Entropy Loss', width * 3 / 4, 45);
            ctx.font = '12px Arial';
            ctx.fillText('L = -Œ£(y¬∑log(≈∑))', width * 3 / 4, 90);
            ctx.fillText('Classification', width * 3 / 4, 115);
        }

        function drawOptimizerSteps(ctx, canvas) {
            const width = canvas.width;
            const height = canvas.height;
            const centerY = height / 2;

            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 16px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('SGD', width / 4, 50);
            ctx.font = '12px Arial';
            ctx.fillText('w = w - Œ±¬∑‚àáL', width / 4, 100);

            ctx.fillStyle = '#00ff88';
            ctx.font = 'bold 16px Arial';
            ctx.fillText('Momentum', width / 2, 50);
            ctx.font = '12px Arial';
            ctx.fillText('v = Œ≤¬∑v + (1-Œ≤)¬∑‚àáL', width / 2, 100);

            ctx.fillStyle = '#ffa500';
            ctx.font = 'bold 16px Arial';
            ctx.fillText('Adam', width * 3 / 4, 50);
            ctx.font = '12px Arial';
            ctx.fillText('Adaptive learning rate', width * 3 / 4, 100);
        }

        function drawChainRule(ctx, canvas) {
            const width = canvas.width;
            ctx.fillStyle = '#00ff88';
            ctx.font = 'bold 16px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Backpropagation Chain Rule', width / 2, 50);
            ctx.font = '12px Arial';
            ctx.fillStyle = '#00d4ff';
            ctx.fillText('dL/dW = dL/d≈∑ ¬∑ d≈∑/da ¬∑ da/dz ¬∑ dz/dW', width / 2, 100);
            ctx.fillStyle = '#ffa500';
            ctx.fillText('Compute gradient by multiplying partial derivatives', width / 2, 150);
        }

        function drawConvolutionMath(ctx, canvas) {
            ctx.fillStyle = '#ff6b35';
            ctx.font = 'bold 16px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Convolution Operation', canvas.width / 2, 50);
            ctx.font = '12px Arial';
            ctx.fillStyle = '#00d4ff';
            ctx.fillText('y[i,j] = Œ£ Œ£ w[m,n] * x[i+m,j+n] + b', canvas.width / 2, 100);
            ctx.fillStyle = '#00ff88';
            ctx.fillText('Sliding window element-wise multiplication and summation', canvas.width / 2, 150);
        }

        function drawPoolingMath(ctx, canvas) {
            const width = canvas.width;
            ctx.fillStyle = '#00ff88';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Max Pooling', width / 3, 50);
            ctx.font = '12px Arial';
            ctx.fillText('y = max(neighborhood)', width / 3, 100);

            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 14px Arial';
            ctx.fillText('Average Pooling', width * 2 / 3, 50);
            ctx.font = '12px Arial';
            ctx.fillText('y = avg(neighborhood)', width * 2 / 3, 100);

            ctx.fillStyle = '#ffa500';
            ctx.font = '11px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Reduces spatial dimensions', width / 2, 150);
        }

        function drawRegularizationMath(ctx, canvas) {
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('L1 Regularization: L = Loss + ŒªŒ£|w|', canvas.width / 2, 60);
            ctx.fillStyle = '#00ff88';
            ctx.fillText('L2 Regularization: L = Loss + ŒªŒ£w¬≤', canvas.width / 2, 110);
            ctx.fillStyle = '#ffa500';
            ctx.fillText('Prevents overfitting by penalizing large weights', canvas.width / 2, 160);
        }

        function drawAttentionMath(ctx, canvas) {
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Attention Mechanism', canvas.width / 2, 50);
            ctx.font = '12px Arial';
            ctx.fillStyle = '#00ff88';
            ctx.fillText('Attention(Q,K,V) = softmax(QK^T/‚àöd_k) ¬∑ V', canvas.width / 2, 100);
            ctx.fillStyle = '#ffa500';
            ctx.fillText('Query-Key matching determines how much to focus on each value', canvas.width / 2, 150);
        }

        function drawRNNMath(ctx, canvas) {
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('RNN Hidden State Update', canvas.width / 2, 50);
            ctx.font = '12px Arial';
            ctx.fillStyle = '#00ff88';
            ctx.fillText('h_t = œÉ(W_h¬∑h_(t-1) + W_x¬∑x_t + b)', canvas.width / 2, 100);
            ctx.fillStyle = '#ffa500';
            ctx.fillText('Processes sequences step-by-step with recurrent connections', canvas.width / 2, 150);
        }

        // Application visualization helper functions
        function drawNNApplications(ctx, canvas) {
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('üì± Stock Price Prediction', canvas.width / 4, 60);
            ctx.fillStyle = '#00ff88';
            ctx.fillText('üè• Medical Diagnosis', canvas.width / 2, 60);
            ctx.fillStyle = '#ffa500';
            ctx.fillText('üéÆ Game AI', canvas.width * 3 / 4, 60);

            ctx.fillStyle = '#ff6b35';
            ctx.font = '12px Arial';
            ctx.fillText('Fraud Detection', canvas.width / 4, 120);
            ctx.fillStyle = '#00d4ff';
            ctx.fillText('Recommendation Systems', canvas.width / 2, 120);
            ctx.fillStyle = '#00ff88';
            ctx.fillText('Credit Scoring', canvas.width * 3 / 4, 120);
        }

        function drawCNNApplications(ctx, canvas) {
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Image Classification', canvas.width / 3, 60);
            ctx.fillStyle = '#00ff88';
            ctx.fillText('Object Detection', canvas.width * 2 / 3, 60);

            ctx.fillStyle = '#ffa500';
            ctx.font = '12px Arial';
            ctx.fillText('Deep Learning Backbone', canvas.width / 2, 150);
        }

        function drawConvolutionApplications(ctx, canvas) {
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('üì∑ Image Feature Extraction', canvas.width / 3, 60);
            ctx.fillStyle = '#00ff88';
            ctx.fillText('üîç Edge Detection', canvas.width * 2 / 3, 60);

            ctx.fillStyle = '#ffa500';
            ctx.font = '12px Arial';
            ctx.fillText('Foundation of Computer Vision', canvas.width / 2, 150);
        }

        function drawYOLOApplications(ctx, canvas) {
            ctx.fillStyle = '#ff6b35';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('üöó Autonomous Driving', canvas.width / 3, 60);
            ctx.fillStyle = '#00d4ff';
            ctx.fillText('üìπ Real-time Video Detection', canvas.width * 2 / 3, 60);

            ctx.fillStyle = '#00ff88';
            ctx.font = '12px Arial';
            ctx.fillText('Ultra-fast inference for live applications', canvas.width / 2, 150);
        }

        function drawSegmentationApplications(ctx, canvas) {
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('üè• Medical Imaging', canvas.width / 3, 60);
            ctx.fillStyle = '#00ff88';
            ctx.fillText('üöó Autonomous Vehicles', canvas.width * 2 / 3, 60);

            ctx.fillStyle = '#ffa500';
            ctx.font = '12px Arial';
            ctx.fillText('Pixel-level understanding of scenes', canvas.width / 2, 150);
        }

        function drawInstanceSegmentationApps(ctx, canvas) {
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('üë• Person Detection & Tracking', canvas.width / 3, 60);
            ctx.fillStyle = '#00ff88';
            ctx.fillText('üçé Object Instance Counting', canvas.width * 2 / 3, 60);

            ctx.fillStyle = '#ffa500';
            ctx.font = '12px Arial';
            ctx.fillText('Separates overlapping objects', canvas.width / 2, 150);
        }

        function drawFaceRecognitionApps(ctx, canvas) {
            ctx.fillStyle = '#ffa500';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('üì± Phone Unlock', canvas.width / 3, 60);
            ctx.fillStyle = '#00d4ff';
            ctx.fillText('üîí Security Systems', canvas.width * 2 / 3, 60);

            ctx.fillStyle = '#00ff88';
            ctx.font = '12px Arial';
            ctx.fillText('Identity verification and access control', canvas.width / 2, 150);
        }

        function drawTransformerApps(ctx, canvas) {
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('üí¨ ChatGPT / LLMs', canvas.width / 3, 60);
            ctx.fillStyle = '#00ff88';
            ctx.fillText('üåê Machine Translation', canvas.width * 2 / 3, 60);

            ctx.fillStyle = '#ffa500';
            ctx.font = '12px Arial';
            ctx.fillText('Foundation of modern NLP and beyond', canvas.width / 2, 150);
        }

        function drawBERTApplications(ctx, canvas) {
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('üîç Semantic Search', canvas.width / 3, 60);
            ctx.fillStyle = '#00ff88';
            ctx.fillText('‚ùì Question Answering', canvas.width * 2 / 3, 60);

            ctx.fillStyle = '#ffa500';
            ctx.font = '12px Arial';
            ctx.fillText('Deep language understanding', canvas.width / 2, 150);
        }

        function drawGPTApplications(ctx, canvas) {
            ctx.fillStyle = '#ff6b35';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('‚úçÔ∏è Text Generation', canvas.width / 3, 60);
            ctx.fillStyle = '#00d4ff';
            ctx.fillText('üí° Idea Assistance', canvas.width * 2 / 3, 60);

            ctx.fillStyle = '#00ff88';
            ctx.font = '12px Arial';
            ctx.fillText('Powerful autoregressive language models', canvas.width / 2, 150);
        }

        function drawGANApplications(ctx, canvas) {
            ctx.fillStyle = '#ff6b35';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('üé® Image Generation', canvas.width / 3, 60);
            ctx.fillStyle = '#00d4ff';
            ctx.fillText('üé≠ Style Transfer', canvas.width * 2 / 3, 60);

            ctx.fillStyle = '#00ff88';
            ctx.font = '12px Arial';
            ctx.fillText('Creative content generation and enhancement', canvas.width / 2, 150);
        }

        function drawDiffusionApplications(ctx, canvas) {
            ctx.fillStyle = '#ffa500';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('üñºÔ∏è Image Synthesis', canvas.width / 3, 60);
            ctx.fillStyle = '#00d4ff';
            ctx.fillText('üé¨ Stable Diffusion', canvas.width * 2 / 3, 60);

            ctx.fillStyle = '#00ff88';
            ctx.font = '12px Arial';
            ctx.fillText('State-of-the-art generative AI', canvas.width / 2, 150);
        }

        // Missing visualization stub functions
        function drawNeuronAnimation(ctx, canvas) {
            drawNetworkGraph(ctx, canvas);
        }

        function drawDecisionBoundary(ctx, canvas) {
            const centerX = canvas.width / 2;
            const centerY = canvas.height / 2;

            // Draw decision boundary line
            ctx.strokeStyle = '#ff6b35';
            ctx.lineWidth = 3;
            ctx.beginPath();
            ctx.moveTo(0, centerY);
            ctx.lineTo(canvas.width, centerY);
            ctx.stroke();

            // Draw sample points
            for (let i = 0; i < 20; i++) {
                const x = Math.random() * canvas.width;
                const y = Math.random() * canvas.height;
                ctx.fillStyle = y < centerY ? '#00d4ff' : '#00ff88';
                ctx.beginPath();
                ctx.arc(x, y, 5, 0, Math.PI * 2);
                ctx.fill();
            }
        }

        function drawWeightDistribution(ctx, canvas) {
            const centerX = canvas.width / 2;
            const centerY = canvas.height / 2;

            // Draw Gaussian distribution
            ctx.strokeStyle = '#00d4ff';
            ctx.lineWidth = 2;
            ctx.beginPath();
            for (let x = -100; x <= 100; x += 2) {
                const y = Math.exp(-(x * x) / 500) * 80;
                const canvasX = centerX + x;
                const canvasY = centerY - y;
                if (x === -100) ctx.moveTo(canvasX, canvasY);
                else ctx.lineTo(canvasX, canvasY);
            }
            ctx.stroke();

            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Weight Distribution (Xavier/He Init)', centerX, 50);
        }

        function drawConvergencePaths(ctx, canvas) {
            drawLossLandscape(ctx, canvas);
        }

        function drawGradientFlow(ctx, canvas) {
            drawChainRule(ctx, canvas);
        }

        function drawOverfitComparison(ctx, canvas) {
            const width = canvas.width;
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Without Regularization', width / 4, 40);
            ctx.fillStyle = '#ff6b35';
            ctx.fillText('With Regularization', width * 3 / 4, 40);

            // Draw wavy overfit line
            ctx.strokeStyle = '#00d4ff';
            ctx.lineWidth = 2;
            ctx.beginPath();
            for (let x = 0; x < width / 2 - 20; x += 5) {
                const y = 100 + Math.sin(x / 10) * 30 + Math.random() * 20;
                if (x === 0) ctx.moveTo(x + 20, y);
                else ctx.lineTo(x + 20, y);
            }
            ctx.stroke();

            // Draw smooth regularized line
            ctx.strokeStyle = '#ff6b35';
            ctx.beginPath();
            for (let x = 0; x < width / 2 - 20; x += 5) {
                const y = 100 + Math.sin(x / 20) * 15;
                if (x === 0) ctx.moveTo(x + width / 2 + 20, y);
                else ctx.lineTo(x + width / 2 + 20, y);
            }
            ctx.stroke();
        }

        function drawBatchNormalization(ctx, canvas) {
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Batch Normalization: Œº=0, œÉ¬≤=1', canvas.width / 2, 50);

            // Draw before/after distributions
            ctx.fillStyle = '#ffa500';
            ctx.fillText('Input Distribution', canvas.width / 4, 100);
            ctx.fillStyle = '#00ff88';
            ctx.fillText('Normalized Distribution', canvas.width * 3 / 4, 100);
        }

        function drawImageMatrix(ctx, canvas) {
            const cellSize = 20;
            for (let i = 0; i < 10; i++) {
                for (let j = 0; j < 10; j++) {
                    const intensity = Math.random();
                    ctx.fillStyle = `rgba(0, 212, 255, ${intensity})`;
                    ctx.fillRect(i * cellSize + 100, j * cellSize + 100, cellSize, cellSize);
                }
            }
            ctx.fillStyle = '#e4e6eb';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Image as Matrix (Pixel Values)', canvas.width / 2, 50);
        }

        function drawPoolingDemo(ctx, canvas) {
            const cellSize = 30;
            const matrix = [[12, 20, 30, 0], [8, 12, 2, 0], [34, 70, 37, 4], [112, 100, 25, 12]];

            ctx.fillStyle = '#e4e6eb';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Max Pooling Demo (2x2)', canvas.width / 2, 30);

            // Draw input matrix
            for (let i = 0; i < 4; i++) {
                for (let j = 0; j < 4; j++) {
                    ctx.strokeStyle = '#00d4ff';
                    ctx.strokeRect(50 + j * cellSize, 50 + i * cellSize, cellSize, cellSize);
                    ctx.fillStyle = '#e4e6eb';
                    ctx.font = '10px Arial';
                    ctx.fillText(matrix[i][j], 50 + j * cellSize + cellSize / 2, 50 + i * cellSize + cellSize / 2 + 4);
                }
            }

            // Draw output (max pooled)
            const pooled = [[20, 30], [112, 37]];
            for (let i = 0; i < 2; i++) {
                for (let j = 0; j < 2; j++) {
                    ctx.strokeStyle = '#00ff88';
                    ctx.strokeRect(250 + j * cellSize * 1.5, 70 + i * cellSize * 1.5, cellSize * 1.5, cellSize * 1.5);
                    ctx.fillStyle = '#00ff88';
                    ctx.font = 'bold 12px Arial';
                    ctx.fillText(pooled[i][j], 250 + j * cellSize * 1.5 + cellSize * 0.75, 70 + i * cellSize * 1.5 + cellSize * 0.75 + 5);
                }
            }
        }

        function drawCNNArchitecture(ctx, canvas) {
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 12px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Input', 60, 200);
            ctx.fillText('Conv', 160, 200);
            ctx.fillText('Pool', 260, 200);
            ctx.fillText('Conv', 360, 200);
            ctx.fillText('Pool', 460, 200);
            ctx.fillText('FC', 560, 200);
            ctx.fillText('Output', 660, 200);

            // Draw blocks
            const blocks = [60, 160, 260, 360, 460, 560, 660];
            blocks.forEach((x, i) => {
                const height = i === 0 ? 100 : (i < blocks.length - 2 ? 80 - i * 10 : 60);
                ctx.strokeStyle = '#00d4ff';
                ctx.strokeRect(x - 30, 100, 60, height);
            });
        }

        function drawLearnedFilters(ctx, canvas) {
            ctx.fillStyle = '#e4e6eb';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('CNN Learned Filters', canvas.width / 2, 30);

            const labels = ['Edges', 'Textures', 'Patterns', 'Objects'];
            labels.forEach((label, i) => {
                const x = (i + 1) * canvas.width / 5;
                ctx.fillStyle = '#ff6b35';
                ctx.font = 'bold 12px Arial';
                ctx.fillText(label, x, 80);

                // Draw filter representation
                for (let j = 0; j < 3; j++) {
                    for (let k = 0; k < 3; k++) {
                        const intensity = Math.random();
                        ctx.fillStyle = `rgba(0, 212, 255, ${intensity})`;
                        ctx.fillRect(x - 20 + k * 12, 100 + j * 12, 10, 10);
                    }
                }
            });
        }

        function drawLeNetArchitecture(ctx, canvas) { drawCNNArchitecture(ctx, canvas); }
        function drawAlexNetArchitecture(ctx, canvas) { drawCNNArchitecture(ctx, canvas); }
        function drawVGGArchitecture(ctx, canvas) { drawCNNArchitecture(ctx, canvas); }
        function drawResNetArchitecture(ctx, canvas) { drawCNNArchitecture(ctx, canvas); }
        function drawInceptionModule(ctx, canvas) { drawCNNArchitecture(ctx, canvas); }
        function drawMobileNetArchitecture(ctx, canvas) { drawCNNArchitecture(ctx, canvas); }
        function drawTransferLearning(ctx, canvas) { drawCNNArchitecture(ctx, canvas); }

        function drawBoundingBoxes(ctx, canvas) {
            // Draw sample image
            ctx.fillStyle = 'rgba(0, 212, 255, 0.1)';
            ctx.fillRect(50, 50, 300, 300);

            // Draw bounding boxes
            ctx.strokeStyle = '#ff6b35';
            ctx.lineWidth = 3;
            ctx.strokeRect(100, 100, 150, 150);
            ctx.fillStyle = '#ff6b35';
            ctx.font = 'bold 12px Arial';
            ctx.fillText('Dog 95%', 105, 95);

            ctx.strokeStyle = '#00ff88';
            ctx.strokeRect(180, 200, 100, 80);
            ctx.fillStyle = '#00ff88';
            ctx.fillText('Cat 87%', 185, 195);
        }

        function drawRCNNPipeline(ctx, canvas) { drawBoundingBoxes(ctx, canvas); }
        function drawSSDDetector(ctx, canvas) { drawBoundingBoxes(ctx, canvas); }

        function drawSemanticSegmentation(ctx, canvas) {
            const cellSize = 15;
            const colors = ['rgba(0, 212, 255, 0.5)', 'rgba(255, 107, 53, 0.5)', 'rgba(0, 255, 136, 0.5)'];

            for (let i = 0; i < 20; i++) {
                for (let j = 0; j < 20; j++) {
                    ctx.fillStyle = colors[Math.floor(Math.random() * colors.length)];
                    ctx.fillRect(i * cellSize + 100, j * cellSize + 50, cellSize, cellSize);
                }
            }

            ctx.fillStyle = '#e4e6eb';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Pixel-wise Classification', canvas.width / 2, 30);
        }

        function drawInstanceSegmentation(ctx, canvas) { drawSemanticSegmentation(ctx, canvas); }

        function drawFaceEmbeddings(ctx, canvas) {
            ctx.fillStyle = '#e4e6eb';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Face Embedding Space', canvas.width / 2, 30);

            // Draw embedding vectors
            const faces = 5;
            for (let i = 0; i < faces; i++) {
                const x = 100 + Math.random() * (canvas.width - 200);
                const y = 100 + Math.random() * 200;
                ctx.fillStyle = '#00d4ff';
                ctx.beginPath();
                ctx.arc(x, y, 10, 0, Math.PI * 2);
                ctx.fill();
            }
        }

        function drawAutoencoderArchitecture(ctx, canvas) {
            ctx.fillStyle = '#00d4ff';
            ctx.font = 'bold 12px Arial';
            ctx.textAlign = 'center';

            const stages = ['Input', 'Encoder', 'Latent', 'Decoder', 'Output'];
            stages.forEach((label, i) => {
                const x = (i + 1) * canvas.width / 6;
                ctx.fillText(label, x, 50);
                const height = i === 2 ? 40 : (i === 0 || i === 4 ? 100 : 70);
                ctx.strokeStyle = '#00d4ff';
                ctx.strokeRect(x - 30, 100, 60, height);
            });
        }

        function drawGANsGame(ctx, canvas) {
            ctx.fillStyle = '#ff6b35';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Generator', canvas.width / 3, 50);
            ctx.fillStyle = '#00d4ff';
            ctx.fillText('Discriminator', canvas.width * 2 / 3, 50);

            // DrawGenerator
            ctx.strokeStyle = '#ff6b35';
            ctx.strokeRect(canvas.width / 3 - 50, 100, 100, 100);

            // Draw Discriminator
            ctx.strokeStyle = '#00d4ff';
            ctx.strokeRect(canvas.width * 2 / 3 - 50, 100, 100, 100);

            // Draw arrow
            ctx.strokeStyle = '#00ff88';
            ctx.lineWidth = 2;
            ctx.beginPath();
            ctx.moveTo(canvas.width / 3 + 50, 150);
            ctx.lineTo(canvas.width * 2 / 3 - 50, 150);
            ctx.stroke();
        }

        function drawDiffusionProcess(ctx, canvas) {
            const steps = 5;
            const stepWidth = canvas.width / (steps + 1);

            ctx.fillStyle = '#e4e6eb';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Diffusion Process: From Noise to Image', canvas.width / 2, 30);

            for (let i = 0; i < steps; i++) {
                const x = (i + 1) * stepWidth;
                const noise = 1 - (i / steps);
                ctx.fillStyle = `rgba(0, 212, 255, ${1 - noise})`;
                ctx.fillRect(x - 40, 100, 80, 80);
                ctx.strokeStyle = '#00d4ff';
                ctx.strokeRect(x - 40, 100, 80, 80);
            }
        }

        function drawRNNUnrolled(ctx, canvas) {
            const cells = 5;
            const cellWidth = canvas.width / (cells + 1);

            ctx.fillStyle = '#e4e6eb';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Unrolled RNN', canvas.width / 2, 30);

            for (let i = 0; i < cells; i++) {
                const x = (i + 1) * cellWidth;
                ctx.strokeStyle = '#00d4ff';
                ctx.strokeRect(x - 30, 100, 60, 60);

                if (i < cells - 1) {
                    ctx.strokeStyle = '#ff6b35';
                    ctx.lineWidth = 2;
                    ctx.beginPath();
                    ctx.moveTo(x + 30, 130);
                    ctx.lineTo(x + cellWidth - 30, 130);
                    ctx.stroke();
                }
            }
        }

        function drawBERTProcess(ctx, canvas) { drawAttentionMatrix(ctx, canvas); }
        function drawGPTGeneration(ctx, canvas) { drawAttentionMatrix(ctx, canvas); }
        function drawVisionTransformer(ctx, canvas) { drawAttentionMatrix(ctx, canvas); }

        function drawVisualization(moduleId) {
            drawConceptsVisualization(moduleId);
        }

        // Animation and download utilities
        function toggleVizAnimation(moduleId) {
            window.vizAnimating = !window.vizAnimating;
            if (window.vizAnimating) {
                animateVisualization(moduleId);
            }
        }

        function animateVisualization(moduleId) {
            if (!window.vizAnimating) return;
            drawConceptsVisualization(moduleId);
            setTimeout(() => animateVisualization(moduleId), 150);
        }

        function downloadViz(moduleId) {
            const canvas = document.getElementById(moduleId + '-canvas');
            if (!canvas) return;

            const link = document.createElement('a');
            link.href = canvas.toDataURL('image/png');
            link.download = moduleId + '-visualization.png';
            link.click();
        }

        initDashboard();
    </script>
</body>

</html>